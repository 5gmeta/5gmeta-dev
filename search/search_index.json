{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"5GMETA Platform User Documentation What is the 5GMETA platform? The 5GMETA open platform aims to leverage car-captured data to stimulate, facilitate and feed innovative products and services. This platform intends to catalyse data and provide OEMs, SMEs and high-tech start-ups with them in order to feed their innovative services and ideas and, ultimately, generate revenues. It is designed to operate as a common infrastructure for implementing data pipelines for heterogeneous Connected, Cooperative and Automated Mobility (CCAM) applications. The 5GMETA platform provides secure end-to-end data management services such as data anonymization, encryption, packaging/formatting, computing and sharing mechanisms. What does the 5GMETA platform do? 5GMETA consortium partners have been working to develop an interactive platform that functions as a broker between data producers (such as vehicles or road sensors) and data consumers (such as app providers). With the 5GMETA Platform, data consumers can subscribe to specific data flows and decide how to use and deploy them: process them, store them or discard them. The 5GMETA Platform allows data to be decoupled from their producers, thus eliminating the necessity of creating direct links between producers and consumers. Nevertheless, the 5GMETA Platform does not only act as a broker, but it also includes functions that address data management, data monetization and cybersecurity. More information can be found in the 5GMETA project website ( https://5gmeta-project.eu/ ). Table of Contents In the next topics we will cover the following topics: Getting started User profiles definition Guide for CAM Application developpers Getting started Consuming guide Dataset Guide Producing Events Guide Data Processing Tutorials Guide for Sensors and Devices Providers Getting started Producing C-ITS data guide Producing images guide Producing video streams guide Receiving events guide Guide for Infrastructure Providers Getting started Instanciating a MEC stack Instanciating low latency data services Additional resources Advanced description Message data broker Stream data gateway Additional examples About","title":"5GMETA Platform User Documentation"},{"location":"#5gmeta-platform-user-documentation","text":"","title":"5GMETA Platform User Documentation"},{"location":"#what-is-the-5gmeta-platform","text":"The 5GMETA open platform aims to leverage car-captured data to stimulate, facilitate and feed innovative products and services. This platform intends to catalyse data and provide OEMs, SMEs and high-tech start-ups with them in order to feed their innovative services and ideas and, ultimately, generate revenues. It is designed to operate as a common infrastructure for implementing data pipelines for heterogeneous Connected, Cooperative and Automated Mobility (CCAM) applications. The 5GMETA platform provides secure end-to-end data management services such as data anonymization, encryption, packaging/formatting, computing and sharing mechanisms.","title":"What is the 5GMETA platform?"},{"location":"#what-does-the-5gmeta-platform-do","text":"5GMETA consortium partners have been working to develop an interactive platform that functions as a broker between data producers (such as vehicles or road sensors) and data consumers (such as app providers). With the 5GMETA Platform, data consumers can subscribe to specific data flows and decide how to use and deploy them: process them, store them or discard them. The 5GMETA Platform allows data to be decoupled from their producers, thus eliminating the necessity of creating direct links between producers and consumers. Nevertheless, the 5GMETA Platform does not only act as a broker, but it also includes functions that address data management, data monetization and cybersecurity. More information can be found in the 5GMETA project website ( https://5gmeta-project.eu/ ).","title":"What does the 5GMETA platform do?"},{"location":"#table-of-contents","text":"In the next topics we will cover the following topics: Getting started User profiles definition Guide for CAM Application developpers Getting started Consuming guide Dataset Guide Producing Events Guide Data Processing Tutorials Guide for Sensors and Devices Providers Getting started Producing C-ITS data guide Producing images guide Producing video streams guide Receiving events guide Guide for Infrastructure Providers Getting started Instanciating a MEC stack Instanciating low latency data services Additional resources Advanced description Message data broker Stream data gateway Additional examples About","title":"Table of Contents"},{"location":"about/","text":"This documentation user guide is created by 5GMETA Consortium partners. List of contributors: Pierre Merdrignac ( pierre.merdrignac@vedecom.fr ) Khaled Chikh ( khaledchikh@unimore.it ) Felipe Mogollon ( fmogollon@vicomtech.org )","title":"About"},{"location":"activemq-example/","text":"Produce and Consumer Examples Description As introduced in the previous section, /activemq_clients provides examples how to produce (cits/image) data from a Sensor&Device to the MEC using python AMQP Qpid Proton reactor API with ActiveMQ. In this section, we will explain in further detail how to implement a data producer. Specific examples will then be provided in the following pages of this documentation. Required packages Linux distribution Python version 3.5+ You have successfully installed python-qpid-proton - including any of its dependencies Refering the examples presented, following are some essential parameters while producing data on the 5GMETA platform: source_id : a Unique Identifier to distinguish the source of generated data. tile : Tile of the source from where the data is being generated in form of QuadKey code. e.g. 1230123012301230 (must be 18 chars in [0-3]) datatype : should be one of the allowed datatype [cits, video, image] sub_datatype : depends upon on the datatype e.g. cam, denm, mappem Generic producer structure A typical producer will contain the following fields, as it can be seen in the examples : Discovery Registration API : This API helps you connect your S&Ds and push data to the MEC within a specified tile. Getting tile of the source from its current GPS position: tileTmp = Tile.for_latitude_longitude(latitude=latitude, longitude=longitude, zoom=18) Getting the message-broker access from the MEC within the previous tile : service=\"message-broker\" messageBroker_ip, messageBroker_port = discovery_registration.discover_sb_service(tile,service) Getting AMQP Topic and dataFlowId to push data into the Message Broker : dataflowId, topic = discovery_registration.register(dataflowmetadata,tile) opts.address=\"amqp://\"+username+\":\"+password+\"@\"+messageBroker_ip+\":\"+str(messageBroker_port)+\":/topic://\"+topic jargs = json.dumps(args) Usage Let's take an example of CITS message producer as shown here in sender.py for reference. Pass the latitude and longitude GPS position of your sensor device as shown here in : # Geoposition - Next steps: from GPS device. latitude = 43.3128 longitude = -1.9750 Replace with your metadata in this section shown below. dataflowmetadata = { \"dataTypeInfo\": { \"dataType\": \"cits\", \"dataSubType\": \"json\" }, \"dataInfo\": { \"dataFormat\": \"asn1_jer\", \"dataSampleRate\": 0.0, \"dataflowDirection\": \"upload\", \"extraAttributes\": None, }, \"licenseInfo\": { \"licenseGeolimit\": \"europe\", \"licenseType\": \"profit\" }, \"dataSourceInfo\": { \"sourceTimezone\": 2, \"sourceStratumLevel\": 3, \"sourceId\": 1, \"sourceType\": \"vehicle\", \"sourceLocationInfo\": { \"locationQuadkey\": tile, \"locationCountry\": \"ESP\", \"locationLatitude\": latitude, \"locationLongitude\": longitude } } } Use the sample content.py to generate your messages. Here as you can see the *msgbody contains the CITS message. def messages_generator(num, tile, msgbody='body_cits_message'): messages.clear() #print(\"Sender prepare the messages... \") for i in range(num): props = { \"dataType\": \"cits\", \"dataSubType\": \"cam\", \"dataFormat\":\"asn1_jer\", \"sourceId\": 1, \"locationQuadkey\": tile+str(i%4), \"body_size\": str(sys.getsizeof(msgbody)) } messages.append( Message(body=msgbody, properties=props) )","title":"Using API for sensors and devices with AMQP"},{"location":"activemq-example/#produce-and-consumer-examples","text":"","title":"Produce and Consumer Examples"},{"location":"activemq-example/#description","text":"As introduced in the previous section, /activemq_clients provides examples how to produce (cits/image) data from a Sensor&Device to the MEC using python AMQP Qpid Proton reactor API with ActiveMQ. In this section, we will explain in further detail how to implement a data producer. Specific examples will then be provided in the following pages of this documentation.","title":"Description"},{"location":"activemq-example/#required-packages","text":"Linux distribution Python version 3.5+ You have successfully installed python-qpid-proton - including any of its dependencies Refering the examples presented, following are some essential parameters while producing data on the 5GMETA platform: source_id : a Unique Identifier to distinguish the source of generated data. tile : Tile of the source from where the data is being generated in form of QuadKey code. e.g. 1230123012301230 (must be 18 chars in [0-3]) datatype : should be one of the allowed datatype [cits, video, image] sub_datatype : depends upon on the datatype e.g. cam, denm, mappem","title":"Required packages"},{"location":"activemq-example/#generic-producer-structure","text":"A typical producer will contain the following fields, as it can be seen in the examples : Discovery Registration API : This API helps you connect your S&Ds and push data to the MEC within a specified tile. Getting tile of the source from its current GPS position: tileTmp = Tile.for_latitude_longitude(latitude=latitude, longitude=longitude, zoom=18) Getting the message-broker access from the MEC within the previous tile : service=\"message-broker\" messageBroker_ip, messageBroker_port = discovery_registration.discover_sb_service(tile,service) Getting AMQP Topic and dataFlowId to push data into the Message Broker : dataflowId, topic = discovery_registration.register(dataflowmetadata,tile) opts.address=\"amqp://\"+username+\":\"+password+\"@\"+messageBroker_ip+\":\"+str(messageBroker_port)+\":/topic://\"+topic jargs = json.dumps(args)","title":"Generic producer structure"},{"location":"activemq-example/#usage","text":"Let's take an example of CITS message producer as shown here in sender.py for reference. Pass the latitude and longitude GPS position of your sensor device as shown here in : # Geoposition - Next steps: from GPS device. latitude = 43.3128 longitude = -1.9750 Replace with your metadata in this section shown below. dataflowmetadata = { \"dataTypeInfo\": { \"dataType\": \"cits\", \"dataSubType\": \"json\" }, \"dataInfo\": { \"dataFormat\": \"asn1_jer\", \"dataSampleRate\": 0.0, \"dataflowDirection\": \"upload\", \"extraAttributes\": None, }, \"licenseInfo\": { \"licenseGeolimit\": \"europe\", \"licenseType\": \"profit\" }, \"dataSourceInfo\": { \"sourceTimezone\": 2, \"sourceStratumLevel\": 3, \"sourceId\": 1, \"sourceType\": \"vehicle\", \"sourceLocationInfo\": { \"locationQuadkey\": tile, \"locationCountry\": \"ESP\", \"locationLatitude\": latitude, \"locationLongitude\": longitude } } } Use the sample content.py to generate your messages. Here as you can see the *msgbody contains the CITS message. def messages_generator(num, tile, msgbody='body_cits_message'): messages.clear() #print(\"Sender prepare the messages... \") for i in range(num): props = { \"dataType\": \"cits\", \"dataSubType\": \"cam\", \"dataFormat\":\"asn1_jer\", \"sourceId\": 1, \"locationQuadkey\": tile+str(i%4), \"body_size\": str(sys.getsizeof(msgbody)) } messages.append( Message(body=msgbody, properties=props) )","title":"Usage"},{"location":"actors_description/","text":"User profiles definition Considering the 5GMETA platform framework, three main profiles to interact with the data platform have been identified: CAM Services and applications developers Sensors and devices providers Infrastructure providers Figure below illustrates the main relation of these different categories with the 5GMETA platform and the principal stakeholders for each of these three profiles. The rest of this manuel is divided in three sections. Depending on how you intend to interact with 5GMETA platform, please select the apprpriate guide: Guide for CAM applications developers Guide for Sensors and devices providers Guide for Infrastructure providers","title":"User profiles definition"},{"location":"actors_description/#user-profiles-definition","text":"Considering the 5GMETA platform framework, three main profiles to interact with the data platform have been identified: CAM Services and applications developers Sensors and devices providers Infrastructure providers Figure below illustrates the main relation of these different categories with the 5GMETA platform and the principal stakeholders for each of these three profiles. The rest of this manuel is divided in three sections. Depending on how you intend to interact with 5GMETA platform, please select the apprpriate guide: Guide for CAM applications developers Guide for Sensors and devices providers Guide for Infrastructure providers","title":"User profiles definition"},{"location":"cits-sender-message-data-broker/","text":"In this page, we introduce examples which can be run for C-ITS data type. Steps to run examples The examples can be found in the folder examples/message-data-broker/cits_sender_python . Modify address.py to put the appropriate ip , port and topic given by your message broker or run with options: Additional arguments as highlighted below could be parsed to the sender.py : ``` h, --help show this help message and exit a ADDRESS, --address=ADDRESS = address to which messages are sent (default amqp:// : @192.168.15.34:5673/topic://cits) m MESSAGES, --messages=MESSAGES = number of messages to send (default 100) t TIMEINTERVAL, --timeinterval=TIMEINTERVAL = messages are sent continuosly every time interval seconds (0: send once) (default 10) ``` In one terminal window run wither of the sender scripts depending upon whether you are running your S&D connected to an database or not . You can add additional arguments as shown before: python3 sender.py or python3 sender_with_sd_database_support.py Example output can be seen below: If you want to do some debugging and check if your messages are being sent run in another terminal to receive messages (you have to modify address.py in order to put the appropriate ip , port and topic given by your message broker ) : Run python3 receiver.py to see the received messages on the subscribed AMQP topic. Use the ActiveMQ admin web page to check Messages Enqueued / Dequeued counts match. You can control which AMQP server the examples try to connect to and the messages they send by changing the values in config.py NB: You have to take into account that any modification made on dataflowmetadata must be applied too into the content.py file in order to generate the appropriate content. Pseudo movement example This example demonstrates data being produced by a moving sensor device. Added some movement around a fixed GPS position in order to simulate movement. Example: cits_send_moving_location.py This way we can move around a MEC that covers tiles: 031333123201033 031333123201211 and a secondary one that covers tiles: 031333123201212 031333123201213 031333123201223 031333123202223 Other resources The initial example for this client is at the link - *https://qpid.apache.org/releases/qpid-proton-0.36.0/proton/python/docs/tutorial.html* - *https://access.redhat.com/documentation/en-us/red_hat_amq/6.3/html/client_connectivity_guide/amqppython*","title":"Sending C-ITS data guide"},{"location":"cits-sender-message-data-broker/#steps-to-run-examples","text":"The examples can be found in the folder examples/message-data-broker/cits_sender_python . Modify address.py to put the appropriate ip , port and topic given by your message broker or run with options: Additional arguments as highlighted below could be parsed to the sender.py : ``` h, --help show this help message and exit a ADDRESS, --address=ADDRESS = address to which messages are sent (default amqp:// : @192.168.15.34:5673/topic://cits) m MESSAGES, --messages=MESSAGES = number of messages to send (default 100) t TIMEINTERVAL, --timeinterval=TIMEINTERVAL = messages are sent continuosly every time interval seconds (0: send once) (default 10) ``` In one terminal window run wither of the sender scripts depending upon whether you are running your S&D connected to an database or not . You can add additional arguments as shown before: python3 sender.py or python3 sender_with_sd_database_support.py Example output can be seen below: If you want to do some debugging and check if your messages are being sent run in another terminal to receive messages (you have to modify address.py in order to put the appropriate ip , port and topic given by your message broker ) : Run python3 receiver.py to see the received messages on the subscribed AMQP topic. Use the ActiveMQ admin web page to check Messages Enqueued / Dequeued counts match. You can control which AMQP server the examples try to connect to and the messages they send by changing the values in config.py NB: You have to take into account that any modification made on dataflowmetadata must be applied too into the content.py file in order to generate the appropriate content.","title":"Steps to run examples"},{"location":"cits-sender-message-data-broker/#pseudo-movement-example","text":"This example demonstrates data being produced by a moving sensor device. Added some movement around a fixed GPS position in order to simulate movement. Example: cits_send_moving_location.py This way we can move around a MEC that covers tiles: 031333123201033 031333123201211 and a secondary one that covers tiles: 031333123201212 031333123201213 031333123201223 031333123202223","title":"Pseudo movement example"},{"location":"cits-sender-message-data-broker/#other-resources","text":"The initial example for this client is at the link - *https://qpid.apache.org/releases/qpid-proton-0.36.0/proton/python/docs/tutorial.html* - *https://access.redhat.com/documentation/en-us/red_hat_amq/6.3/html/client_connectivity_guide/amqppython*","title":"Other resources"},{"location":"consuming-guide/","text":"5GMETA platform is an IoT oriented platform that produces data from vehicles to be consumed by third party applications in order to get data from several types: CITS json like messages with data extracted from the vehicle (ex: GPS position, etc) Example images jpg Video streaming Guide to consuming data Before jumping to consuming the available datatypes, it is necessary to understand some concepts in order to use 5GMETA APIs: Tiles : About Geographic Data Tiles : Geographic data tiles are a way of organizing and distributing large datasets of geographic information, such as satellite imagery or digital maps. Each tile represents a specific geographic area , and the data within each tile is typically organized into a regular grid or matrix. By dividing data into tiles, it becomes easier to manage and distribute large datasets, as users can selectively download only the tiles they need. In this consuming guide file, we provide information about the geographic areas covered by our dataset tiles Datasets available within tiles , along with instructions on how to access and use the data. By using tiles to organize and distribute our data, we aim to make it more accessible and easier to use for a wide range of applications. Available Datatypes : 5GMETA Platform provide wide range of datatypes: C-ITS : Cooperative Intelligent Transport Systems JSON Format, ETSI CAM messages also included (ex: GPS position, speed, etc..) ETSI is the European Telecommunications Standards Institute (Vehicular communication is based on wireless Vehicle-2-Everything (V2X) networks) Images : JPG Format Video Streams Video Streams are in H.264 video standard Format (x264 implementation) Instance Types : 5GMETA Platform offers a range of computing resources for users to choose from. Instance types refer to different configurations of computing resources that are available for users to choose from when setting up their virtual machines or cloud computing instances. In this example, we have four different instance types with varying amounts of CPU, GPU, and memory resources: \" small \" instance type: This instance type has 2 CPUs, 2GB of memory, and no GPU. \" medium \" instance type: This instance type has 4 CPUs, 4GB of memory, and no GPU. \" large \" instance type: This instance type has 8 CPUs, 8GB of memory, and no GPU. \" advanced \" instance type: This instance type has 8 CPUs, 8GB of memory, and a GPU. The instance types are identified by unique type IDs (type_id) and human-readable names (type_name) to make it easier for users to select the type of instance that best suits their needs. Software requirements This guide is oriented to be executed in an Ubuntu 20.04 environment. Extra packages to be installed First of all, you will need to install some dependencies (apt-get): python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad (only if you are going to consume video) gstreamer1.0-libav (only if you are going to consume video) python3-gst-1.0 (only if you are going to consume video) sudo apt-get install python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad gstreamer1.0-libav python3-gst-1.0 Also install with pip3: kafka-python numpy python-qpid-proton requests confluent-kafka avro Note : Be careful of your environment compatibility: pip3 install -r examples/stream-data-gateway/requirements.txt Link for examples/stream-data-gateway/requirements.txt Platform-client helper application There is a guided applicaction that will help you to get the apropriate parameters from 5GMETA platform to get the data you need. You can execute it by downloading all content from folder: Platform client helper application Once you have donwload that software you can run it by executing: $ python3 client.py in your command line. Client usage Once you have executed the previous command you will be prompted for: * 5GMETA username * 5GMETA password * These credentials will be valid if you have registered to 5GMETA platform offered by the Identity building block. During 5GMETA project, registration can be done at this Registration web page . After entering your username/password, client will ask you if you want to: * Consume data from 5GMETA platform * Produce an event in a vehicle connected to 5GMETA platform In our case we have decided to consume data, so we push c. Inmediately client will show which tile have data. And will ask you to select one of them to consume data from. You can stop selecting tiles by pushing q in your keyboard. After selecting tiles client will show which datatype is available in the tiles you have selected. And will ask you to select which datatype do you want to consume. Once selected you will be prompted with the instancetype you can use in the MEC that is managin data from that tile and will be asked to choose one. Once selected you will be prompted with the parameters from 5GMETA platform you have to use in consumer examples Please notice that the parameters showed in that ouput ARE VALID ONLY IF YOU KEEP client.py RUNNING , once you stop it by pressing q those parameters could not be valid for your consumer application. PLEASE DON'T STOP client.py APPLICATION BY PUSHING CTRL-C Consumer examples 5GMETA platform offers some examples to comsume those data. CITS command line consumer image command line consumer video command line consumer Those consumer clients will ask you for the parameters obtained as output in the Client usage section. CITS consumer This client is a Kafka client that will consume CITS data from 5GMETA platform and will print on the command line output. It takes as input parameters: Kafka topic Kafka broker address Kafka bootstrap port Kafka schema registry port IMAGE consumer This client is a Kafka client that will consume images data from 5GMETA platform and will print on the command line output. It takes as input parameters: Kafka topic Kafka broker address Kafka bootstrap port Kafka schema registry port Consumer instructions Select the suitable consumer as per the produced data and use as follows: python3 cits-consumer.py topic platformaddress bootstrap_port registry_port or mkdir output python3 image-consumer.py topic platformaddress bootstrap_port registry_port or python3 video-consumer.py platformaddress bootstrap_port topic dataflow_id","title":"Consuming Guide"},{"location":"consuming-guide/#guide-to-consuming-data","text":"Before jumping to consuming the available datatypes, it is necessary to understand some concepts in order to use 5GMETA APIs: Tiles : About Geographic Data Tiles : Geographic data tiles are a way of organizing and distributing large datasets of geographic information, such as satellite imagery or digital maps. Each tile represents a specific geographic area , and the data within each tile is typically organized into a regular grid or matrix. By dividing data into tiles, it becomes easier to manage and distribute large datasets, as users can selectively download only the tiles they need. In this consuming guide file, we provide information about the geographic areas covered by our dataset tiles Datasets available within tiles , along with instructions on how to access and use the data. By using tiles to organize and distribute our data, we aim to make it more accessible and easier to use for a wide range of applications. Available Datatypes : 5GMETA Platform provide wide range of datatypes: C-ITS : Cooperative Intelligent Transport Systems JSON Format, ETSI CAM messages also included (ex: GPS position, speed, etc..) ETSI is the European Telecommunications Standards Institute (Vehicular communication is based on wireless Vehicle-2-Everything (V2X) networks) Images : JPG Format Video Streams Video Streams are in H.264 video standard Format (x264 implementation) Instance Types : 5GMETA Platform offers a range of computing resources for users to choose from. Instance types refer to different configurations of computing resources that are available for users to choose from when setting up their virtual machines or cloud computing instances. In this example, we have four different instance types with varying amounts of CPU, GPU, and memory resources: \" small \" instance type: This instance type has 2 CPUs, 2GB of memory, and no GPU. \" medium \" instance type: This instance type has 4 CPUs, 4GB of memory, and no GPU. \" large \" instance type: This instance type has 8 CPUs, 8GB of memory, and no GPU. \" advanced \" instance type: This instance type has 8 CPUs, 8GB of memory, and a GPU. The instance types are identified by unique type IDs (type_id) and human-readable names (type_name) to make it easier for users to select the type of instance that best suits their needs.","title":"Guide to consuming data"},{"location":"consuming-guide/#software-requirements","text":"This guide is oriented to be executed in an Ubuntu 20.04 environment.","title":"Software requirements"},{"location":"consuming-guide/#extra-packages-to-be-installed","text":"First of all, you will need to install some dependencies (apt-get): python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad (only if you are going to consume video) gstreamer1.0-libav (only if you are going to consume video) python3-gst-1.0 (only if you are going to consume video) sudo apt-get install python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad gstreamer1.0-libav python3-gst-1.0 Also install with pip3: kafka-python numpy python-qpid-proton requests confluent-kafka avro Note : Be careful of your environment compatibility: pip3 install -r examples/stream-data-gateway/requirements.txt Link for examples/stream-data-gateway/requirements.txt","title":"Extra packages to be installed"},{"location":"consuming-guide/#platform-client-helper-application","text":"There is a guided applicaction that will help you to get the apropriate parameters from 5GMETA platform to get the data you need. You can execute it by downloading all content from folder: Platform client helper application Once you have donwload that software you can run it by executing: $ python3 client.py in your command line.","title":"Platform-client helper application"},{"location":"consuming-guide/#client-usage","text":"Once you have executed the previous command you will be prompted for: * 5GMETA username * 5GMETA password * These credentials will be valid if you have registered to 5GMETA platform offered by the Identity building block. During 5GMETA project, registration can be done at this Registration web page . After entering your username/password, client will ask you if you want to: * Consume data from 5GMETA platform * Produce an event in a vehicle connected to 5GMETA platform In our case we have decided to consume data, so we push c. Inmediately client will show which tile have data. And will ask you to select one of them to consume data from. You can stop selecting tiles by pushing q in your keyboard. After selecting tiles client will show which datatype is available in the tiles you have selected. And will ask you to select which datatype do you want to consume. Once selected you will be prompted with the instancetype you can use in the MEC that is managin data from that tile and will be asked to choose one. Once selected you will be prompted with the parameters from 5GMETA platform you have to use in consumer examples Please notice that the parameters showed in that ouput ARE VALID ONLY IF YOU KEEP client.py RUNNING , once you stop it by pressing q those parameters could not be valid for your consumer application. PLEASE DON'T STOP client.py APPLICATION BY PUSHING CTRL-C","title":"Client usage"},{"location":"consuming-guide/#consumer-examples","text":"5GMETA platform offers some examples to comsume those data. CITS command line consumer image command line consumer video command line consumer Those consumer clients will ask you for the parameters obtained as output in the Client usage section.","title":"Consumer examples"},{"location":"consuming-guide/#cits-consumer","text":"This client is a Kafka client that will consume CITS data from 5GMETA platform and will print on the command line output. It takes as input parameters: Kafka topic Kafka broker address Kafka bootstrap port Kafka schema registry port","title":"CITS consumer"},{"location":"consuming-guide/#image-consumer","text":"This client is a Kafka client that will consume images data from 5GMETA platform and will print on the command line output. It takes as input parameters: Kafka topic Kafka broker address Kafka bootstrap port Kafka schema registry port","title":"IMAGE consumer"},{"location":"consuming-guide/#consumer-instructions","text":"Select the suitable consumer as per the produced data and use as follows: python3 cits-consumer.py topic platformaddress bootstrap_port registry_port or mkdir output python3 image-consumer.py topic platformaddress bootstrap_port registry_port or python3 video-consumer.py platformaddress bootstrap_port topic dataflow_id","title":"Consumer instructions"},{"location":"datasets/","text":"Sample dataset to test a CAM application You will find below a list of datasets which can be used to help in the development of a CAM application. 5GMETA DATA TYPE DESCRIPTION TILE SAMPLE ReadMe C-ITS Simulation Cooperative Awareness Messages (position, heading, speed, acceleration, etc...) from simulated vehicles anywhere/worldwide WebApp UI README C-ITS Position, heading, speed and acceleration from vehicles in Donostia (Spain) 0313331232 cits-vicomtech-donostia.json README C-ITS Position, heading, speed and acceleration from vehicles in Toulouse (France) 120222021 cits-vicomtech-toulouse.json README C-ITS Category, Position, orientation, speed and cam_id, object_id from vehicles, person, bycicle..etc in MODENA (Italy) 1202231113220102 cits-unimore-modena-masa.json README C-ITS Camera_id, space_id ,space position, Occupied Or Empty, from Parking Lots in MODENA (Italy) 1202231113220102 cits-unimore-modena-pld.json README C-ITS GPS Position, dms_level, dms_trigger from Driver monitoring system in MODENA (Italy) 1202231113220102 cits-unimore-modena-dms.json README C-ITS Position, heading, speed and acceleration from vehicles in MODENA (Italy) 1202231113220102 cits-unimore-modena-cam.json README image jpg from Donostia (Spain) 0313331232 image-sample-vicomtech-donostia.jpg README image jpg from Toulouse (France) 120222021 image-sample-vicomtech-toulouse.jpg README C-ITS Position, heading, speed and acceleration from vehicles in Versailles (France) 1202200101311 cits-versailles_area-cam.json README C-ITS Status of traffic light in Versailles (France) 1202200101311 cits-versailles_area-spat.json README C-ITS Parking area occupancy in Versailles (France) 1202200101311 cits-versailles_area-pam.json README C-ITS Traffic counting from with 2 metric (Throughput, Occupation rate) with 5 sensors in Versailles (France) 1202200101311 traffic_sensors_samples.json README","title":"Dataset Guide"},{"location":"datasets/#sample-dataset-to-test-a-cam-application","text":"You will find below a list of datasets which can be used to help in the development of a CAM application. 5GMETA DATA TYPE DESCRIPTION TILE SAMPLE ReadMe C-ITS Simulation Cooperative Awareness Messages (position, heading, speed, acceleration, etc...) from simulated vehicles anywhere/worldwide WebApp UI README C-ITS Position, heading, speed and acceleration from vehicles in Donostia (Spain) 0313331232 cits-vicomtech-donostia.json README C-ITS Position, heading, speed and acceleration from vehicles in Toulouse (France) 120222021 cits-vicomtech-toulouse.json README C-ITS Category, Position, orientation, speed and cam_id, object_id from vehicles, person, bycicle..etc in MODENA (Italy) 1202231113220102 cits-unimore-modena-masa.json README C-ITS Camera_id, space_id ,space position, Occupied Or Empty, from Parking Lots in MODENA (Italy) 1202231113220102 cits-unimore-modena-pld.json README C-ITS GPS Position, dms_level, dms_trigger from Driver monitoring system in MODENA (Italy) 1202231113220102 cits-unimore-modena-dms.json README C-ITS Position, heading, speed and acceleration from vehicles in MODENA (Italy) 1202231113220102 cits-unimore-modena-cam.json README image jpg from Donostia (Spain) 0313331232 image-sample-vicomtech-donostia.jpg README image jpg from Toulouse (France) 120222021 image-sample-vicomtech-toulouse.jpg README C-ITS Position, heading, speed and acceleration from vehicles in Versailles (France) 1202200101311 cits-versailles_area-cam.json README C-ITS Status of traffic light in Versailles (France) 1202200101311 cits-versailles_area-spat.json README C-ITS Parking area occupancy in Versailles (France) 1202200101311 cits-versailles_area-pam.json README C-ITS Traffic counting from with 2 metric (Throughput, Occupation rate) with 5 sensors in Versailles (France) 1202200101311 traffic_sensors_samples.json README","title":"Sample dataset to test a CAM application"},{"location":"edge-deployment/","text":"Edge Stack Deployment In order to easily deploy all the components forming the 5GMETA Edge Stack, an unique Ansible playbook will be used. The following playbook will install all the dependencies and components and repositories necessary for the 5GMETA platform's MEC server. Furthermore, every time a change is made in the stack, it will be reflected in the playbook and running it again will let to update the deployment without extra effort. The only requeriment to deploy the stack is an Ubuntu 20.04 image. I can also be an Ubuntu 18.04 image, but in that case OSM 10 will be installed instead OSM 11. The server needs at least 16 GB of RAM, 4 vCPUs and 60 GB of disk. The playbook will the deploy the following components: - Docker Engine - Single-node Kubernetes cluster (v1.20.11) - Composed by Flannel CNI, OpenEBS Storage and MetalLB Load-balancer - Helm k8s application manager - Kube-prometheus-stack and Kube-eagle for k8s monitoring - Prometheus, Grafana and dashboards - MySQL cluster - Open Source MANO (OSM): v11 for Ubuntu 20.04 and v10 for Ubuntu 18.04 - Notary and Connaisseur for managing security in the cluster - 5GMETA MEC APIs and Base Components Installing requirements To run the Ansible playbook, some packages & Ansible collections must be installed before in the control node: sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt-get install curl python3-pip ansible ansible-galaxy collection install community.general community.docker kubernetes.core In Ubuntu 18.04: sudo apt-get install python3-distutils python3 get-pip.py python3 -m pip install ansible ansible-galaxy collection install community.general community.docker kubernetes.core For RP1, the public IP of the server should be whitelisted, please contact Vicomtech to add it to the list. Deploying the Edge Stack After installing the previous requirements, the Ansible playbook should be downloaded: curl -s https://<auth-token>@raw.githubusercontent.com/5gmeta/orchestrator/main/deploy/EdgeDeployment.yaml -o EdgeDeployment.yaml Then, before running it, the playbook's hosts field or Ansible's inventory file must be modified to match the Ubuntu machine. Vars section must be also filled with the user provided values. After that it can be runned: ansible-playbook EdgeDeployment.yaml Once deployed you can access the different services in the next ports: - K8s API in port 6443 - K8s UI in port 8080 - OSM UI in port 80 - OSM API (Orchestration API) in port 9999 - Grafana UI in port 3000 - Grafana/Loki UI in port 7000 - Prometheus UI in port 9090 - Alert Manager UI in port 9093 - 5GMETA Edge Instance API in port 5000 - 5GMETA Registration API in port 12346 - 5GMETA Message-Broker in port 5673 (SB) and 61616 (NB) - 5GMETA Video-Broker in port 8443 Also you can check the status of OSM ressources managed by Kubernetes in the following way: kubectl get all -n osm Re-deploying starting from a specific task of the Ansible playbook If the deployement fails for any reason and you want to play again the playbook starting from a specific task, then you do the following: # For running the playbook starting from the \"Get IP geolocation data\" task ansible-playbook EdgeDeployment.yaml --start-at-task=\"Get IP geolocation data\" Enable no username/password in a running MEC $ ansible-playbook EdgeDeployment.yaml --tag \"deploy_k8s_prometheus_grafana\" $ ansible-playbook EdgeDeployment.yaml --tag \"expose_k8s_grafana\" Enable pods log monitoring in a running MEC $ ansible-playbook EdgeDeployment.yaml --tag \"deploy_grafana_loki\"\u200b $ ansible-playbook EdgeDeployment.yaml --tag \"expose_grafana_loki\" Executing only a pecific task of the Ansible playbook If instead you want to execute a single task of the playbook, then you can force Ansible to ask for a confirmation before executing the next step, and abort execution when needed: # For running only the \"Get IP geolocation data\" task, abort execution (ctrl+c) when asked to confirm execution of the following step ansible-playbook playbook.yml --step --start-at-task=\"install packages\" Cleaning deployment To clean the deployment, use the cleanDeployment.sh script. Known/Potential issues related to Ansible playbook execution You are runninng the installer as root This error can be resolved by running the playbook with the current user and not the root user , while having sudo privileges: sudo -u $USER ansible-playbook EdgeDeployment.yaml --ask-become-pass Couldn't resolve module/action 'community.general.ipinfoio_facts' This error is due to the usage of an old version of ansible, and can be easily resolved through: sudo apt purge ansible sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt install ansible ansible-galaxy collection install community.general community.docker kubernetes.core --force Cannot uninstall 'PyYAML' PyYAML is potentially installed and already bundled into some core packages of the distro. To force installation of required PyYAML version: sudo pip install --ignore-installed PyYAML Known/Potential issues related to Docker Docker-credential-secretservice failure If the playbook fails in Docker login task (e.g Docker-credential-secretservice fails on Ubuntu 18.04: error getting credentials - err: exit status 1, out:GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.secrets was not provided by any .service files :), run the following commannd and then the ansible again: sudo apt install gnupg2 passls -la Known/Potential issues related to Kubernetes Failure of Kubernetes Cluster init due to unvalidated docker version This error is due to the usage of an unvalidated docker version. To fix it add the following arguments like follows: # Not working - kubeadm init --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init # Working - kubeadm init --ignore-preflight-errors=SystemVerification --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init Failure with validating the existence and emptiness of directory /var/lib/etcd This error can be easily fixed by removing the directory: sudo rm /var/lib/etcd Known/Potential issues related to OSM Failure of OSM install with several deployements still pending If OSM installation fails due to pending deployements, like below: 5 of 9 deployments starting: lcm 0/1 0 mon 0/1 0 nbi 0/1 0 pol 0/1 0 ro 0/1 0 SYSTEM IS BROKEN OSM is not healthy, but will probably converge to a healthy state soon. Check OSM status with: kubectl -n osm get all DONE Identifying the root cause can be difficult. Still a potential solution is to check the logs related to such deployements like below: sudo kubectl get all -n osm #command output!!! NAME READY STATUS RESTARTS AGE pod/grafana-855d96c47d-w4xfs 2/2 Running 0 66m pod/kafka-0 1/1 Running 3 66m pod/keystone-7d9864f8f5-qx4dd 1/1 Running 0 66m pod/lcm-556b46f977-c2ljl 0/1 Init:0/1 0 66m pod/modeloperator-946f64449-mffbc 1/1 Running 0 66m pod/mon-577c8fc975-9kndp 0/1 Init:0/1 0 66m pod/mysql-0 1/1 Running 0 66m pod/nbi-7886459c9f-mldhw 0/1 Init:0/1 0 66m pod/ng-ui-7d98c6568f-d8862 1/1 Running 0 66m pod/pol-7f86867d95-vm762 0/1 Init:0/1 0 66m pod/prometheus-0 1/1 Running 0 66m pod/ro-ddbc9f888-6jmlp 0/1 Init:0/1 0 66m pod/zookeeper-0 1/1 Running 0 66m Then it is possible to get details related to a specific deployment: kubectl describe pod/lcm-556b46f977-c2ljl -n osm #command output!!! Name: lcm-556b46f977-c2ljl Namespace: osm Priority: 0 .... .... Init Containers: kafka-ro-mongo-test: Container ID: docker://8c4fb0ce225af5c4419441c21b1493da252441b83fbe963d4ef7125c44389591 Image: alpine:latest Image ID: docker-pullable:// .... .... And finally to check its logs and identify the root cause kubectl logs pod/lcm-556b46f977-c2ljl-n osm -c kafka-ro-mongo-test #command output!!! nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open OSM command returning \"ModuleNotFoundError: No module named 'prettytable' This error can be easily fixed by: sudo apt install python3-prettytable OSM command returning TypeError: init () got an unexpected keyword argument 'case_sensitive' This error can be easily fixed by: pip3 install -U click","title":"Instanciating a MEC stack"},{"location":"edge-deployment/#edge-stack-deployment","text":"In order to easily deploy all the components forming the 5GMETA Edge Stack, an unique Ansible playbook will be used. The following playbook will install all the dependencies and components and repositories necessary for the 5GMETA platform's MEC server. Furthermore, every time a change is made in the stack, it will be reflected in the playbook and running it again will let to update the deployment without extra effort. The only requeriment to deploy the stack is an Ubuntu 20.04 image. I can also be an Ubuntu 18.04 image, but in that case OSM 10 will be installed instead OSM 11. The server needs at least 16 GB of RAM, 4 vCPUs and 60 GB of disk. The playbook will the deploy the following components: - Docker Engine - Single-node Kubernetes cluster (v1.20.11) - Composed by Flannel CNI, OpenEBS Storage and MetalLB Load-balancer - Helm k8s application manager - Kube-prometheus-stack and Kube-eagle for k8s monitoring - Prometheus, Grafana and dashboards - MySQL cluster - Open Source MANO (OSM): v11 for Ubuntu 20.04 and v10 for Ubuntu 18.04 - Notary and Connaisseur for managing security in the cluster - 5GMETA MEC APIs and Base Components","title":"Edge Stack Deployment"},{"location":"edge-deployment/#installing-requirements","text":"To run the Ansible playbook, some packages & Ansible collections must be installed before in the control node: sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt-get install curl python3-pip ansible ansible-galaxy collection install community.general community.docker kubernetes.core In Ubuntu 18.04: sudo apt-get install python3-distutils python3 get-pip.py python3 -m pip install ansible ansible-galaxy collection install community.general community.docker kubernetes.core For RP1, the public IP of the server should be whitelisted, please contact Vicomtech to add it to the list.","title":"Installing requirements"},{"location":"edge-deployment/#deploying-the-edge-stack","text":"After installing the previous requirements, the Ansible playbook should be downloaded: curl -s https://<auth-token>@raw.githubusercontent.com/5gmeta/orchestrator/main/deploy/EdgeDeployment.yaml -o EdgeDeployment.yaml Then, before running it, the playbook's hosts field or Ansible's inventory file must be modified to match the Ubuntu machine. Vars section must be also filled with the user provided values. After that it can be runned: ansible-playbook EdgeDeployment.yaml Once deployed you can access the different services in the next ports: - K8s API in port 6443 - K8s UI in port 8080 - OSM UI in port 80 - OSM API (Orchestration API) in port 9999 - Grafana UI in port 3000 - Grafana/Loki UI in port 7000 - Prometheus UI in port 9090 - Alert Manager UI in port 9093 - 5GMETA Edge Instance API in port 5000 - 5GMETA Registration API in port 12346 - 5GMETA Message-Broker in port 5673 (SB) and 61616 (NB) - 5GMETA Video-Broker in port 8443 Also you can check the status of OSM ressources managed by Kubernetes in the following way: kubectl get all -n osm","title":"Deploying the Edge Stack"},{"location":"edge-deployment/#re-deploying-starting-from-a-specific-task-of-the-ansible-playbook","text":"If the deployement fails for any reason and you want to play again the playbook starting from a specific task, then you do the following: # For running the playbook starting from the \"Get IP geolocation data\" task ansible-playbook EdgeDeployment.yaml --start-at-task=\"Get IP geolocation data\"","title":"Re-deploying starting from a specific task of the Ansible playbook"},{"location":"edge-deployment/#enable-no-usernamepassword-in-a-running-mec","text":"$ ansible-playbook EdgeDeployment.yaml --tag \"deploy_k8s_prometheus_grafana\" $ ansible-playbook EdgeDeployment.yaml --tag \"expose_k8s_grafana\"","title":"Enable no username/password in a running MEC"},{"location":"edge-deployment/#enable-pods-log-monitoring-in-a-running-mec","text":"$ ansible-playbook EdgeDeployment.yaml --tag \"deploy_grafana_loki\"\u200b $ ansible-playbook EdgeDeployment.yaml --tag \"expose_grafana_loki\"","title":"Enable pods log monitoring in a running MEC"},{"location":"edge-deployment/#executing-only-a-pecific-task-of-the-ansible-playbook","text":"If instead you want to execute a single task of the playbook, then you can force Ansible to ask for a confirmation before executing the next step, and abort execution when needed: # For running only the \"Get IP geolocation data\" task, abort execution (ctrl+c) when asked to confirm execution of the following step ansible-playbook playbook.yml --step --start-at-task=\"install packages\"","title":"Executing only a pecific task of the Ansible playbook"},{"location":"edge-deployment/#cleaning-deployment","text":"To clean the deployment, use the cleanDeployment.sh script.","title":"Cleaning deployment"},{"location":"edge-deployment/#knownpotential-issues-related-to-ansible-playbook-execution","text":"","title":"Known/Potential issues related to Ansible playbook execution"},{"location":"edge-deployment/#you-are-runninng-the-installer-as-root","text":"This error can be resolved by running the playbook with the current user and not the root user , while having sudo privileges: sudo -u $USER ansible-playbook EdgeDeployment.yaml --ask-become-pass","title":"You are runninng the installer as root"},{"location":"edge-deployment/#couldnt-resolve-moduleaction-communitygeneralipinfoio_facts","text":"This error is due to the usage of an old version of ansible, and can be easily resolved through: sudo apt purge ansible sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt install ansible ansible-galaxy collection install community.general community.docker kubernetes.core --force","title":"Couldn't resolve module/action 'community.general.ipinfoio_facts'"},{"location":"edge-deployment/#cannot-uninstall-pyyaml","text":"PyYAML is potentially installed and already bundled into some core packages of the distro. To force installation of required PyYAML version: sudo pip install --ignore-installed PyYAML","title":"Cannot uninstall 'PyYAML'"},{"location":"edge-deployment/#knownpotential-issues-related-to-docker","text":"","title":"Known/Potential issues related to Docker"},{"location":"edge-deployment/#docker-credential-secretservice-failure","text":"If the playbook fails in Docker login task (e.g Docker-credential-secretservice fails on Ubuntu 18.04: error getting credentials - err: exit status 1, out:GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.secrets was not provided by any .service files :), run the following commannd and then the ansible again: sudo apt install gnupg2 passls -la","title":"Docker-credential-secretservice failure"},{"location":"edge-deployment/#knownpotential-issues-related-to-kubernetes","text":"","title":"Known/Potential issues related to Kubernetes"},{"location":"edge-deployment/#failure-of-kubernetes-cluster-init-due-to-unvalidated-docker-version","text":"This error is due to the usage of an unvalidated docker version. To fix it add the following arguments like follows: # Not working - kubeadm init --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init # Working - kubeadm init --ignore-preflight-errors=SystemVerification --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init","title":"Failure of Kubernetes Cluster init due to unvalidated docker version"},{"location":"edge-deployment/#failure-with-validating-the-existence-and-emptiness-of-directory-varlibetcd","text":"This error can be easily fixed by removing the directory: sudo rm /var/lib/etcd","title":"Failure with validating the existence and emptiness of directory /var/lib/etcd"},{"location":"edge-deployment/#knownpotential-issues-related-to-osm","text":"","title":"Known/Potential issues related to OSM"},{"location":"edge-deployment/#failure-of-osm-install-with-several-deployements-still-pending","text":"If OSM installation fails due to pending deployements, like below: 5 of 9 deployments starting: lcm 0/1 0 mon 0/1 0 nbi 0/1 0 pol 0/1 0 ro 0/1 0 SYSTEM IS BROKEN OSM is not healthy, but will probably converge to a healthy state soon. Check OSM status with: kubectl -n osm get all DONE Identifying the root cause can be difficult. Still a potential solution is to check the logs related to such deployements like below: sudo kubectl get all -n osm #command output!!! NAME READY STATUS RESTARTS AGE pod/grafana-855d96c47d-w4xfs 2/2 Running 0 66m pod/kafka-0 1/1 Running 3 66m pod/keystone-7d9864f8f5-qx4dd 1/1 Running 0 66m pod/lcm-556b46f977-c2ljl 0/1 Init:0/1 0 66m pod/modeloperator-946f64449-mffbc 1/1 Running 0 66m pod/mon-577c8fc975-9kndp 0/1 Init:0/1 0 66m pod/mysql-0 1/1 Running 0 66m pod/nbi-7886459c9f-mldhw 0/1 Init:0/1 0 66m pod/ng-ui-7d98c6568f-d8862 1/1 Running 0 66m pod/pol-7f86867d95-vm762 0/1 Init:0/1 0 66m pod/prometheus-0 1/1 Running 0 66m pod/ro-ddbc9f888-6jmlp 0/1 Init:0/1 0 66m pod/zookeeper-0 1/1 Running 0 66m Then it is possible to get details related to a specific deployment: kubectl describe pod/lcm-556b46f977-c2ljl -n osm #command output!!! Name: lcm-556b46f977-c2ljl Namespace: osm Priority: 0 .... .... Init Containers: kafka-ro-mongo-test: Container ID: docker://8c4fb0ce225af5c4419441c21b1493da252441b83fbe963d4ef7125c44389591 Image: alpine:latest Image ID: docker-pullable:// .... .... And finally to check its logs and identify the root cause kubectl logs pod/lcm-556b46f977-c2ljl-n osm -c kafka-ro-mongo-test #command output!!! nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open","title":"Failure of OSM install with several deployements still pending"},{"location":"edge-deployment/#osm-command-returning-modulenotfounderror-no-module-named-prettytable","text":"This error can be easily fixed by: sudo apt install python3-prettytable","title":"OSM command returning \"ModuleNotFoundError: No module named 'prettytable'"},{"location":"edge-deployment/#osm-command-returning-typeerror-init-got-an-unexpected-keyword-argument-case_sensitive","text":"This error can be easily fixed by: pip3 install -U click","title":"OSM command returning TypeError: init() got an unexpected keyword argument 'case_sensitive'"},{"location":"getting-started-infrastructure-provider/","text":"MEC plaform deployement The installation of all the SW stack of the MEC infrastructure, including the 5GMETA modules forming the 5GMETA MEC Stack, a unique Ansible playbook shall be used. The Ansible playbook installs all the dependencies and components and repositories necessary for the 5GMETA platform's MEC server. Furthermore, every time a change is made in the stack, it will be reflected in the playbook and running it again will update the deployment without extra effort. The Ansible playbook includes : The baseline/default building blocks of the 5GMETA MEC platform defined in D2.3 (Definition of 5GMETA architecture). The introspection of HW assets to be offered to CCAM applications to process data or to host a low latency service. The register of all the information of the MEC platform in the Discovery service at the 5GMETA Cloud platform. The orchestration and instance type systems to instance pipelines and third-party services on demand. The process is illustrated as follows:","title":"Getting Started"},{"location":"getting-started-infrastructure-provider/#mec-plaform-deployement","text":"The installation of all the SW stack of the MEC infrastructure, including the 5GMETA modules forming the 5GMETA MEC Stack, a unique Ansible playbook shall be used. The Ansible playbook installs all the dependencies and components and repositories necessary for the 5GMETA platform's MEC server. Furthermore, every time a change is made in the stack, it will be reflected in the playbook and running it again will update the deployment without extra effort. The Ansible playbook includes : The baseline/default building blocks of the 5GMETA MEC platform defined in D2.3 (Definition of 5GMETA architecture). The introspection of HW assets to be offered to CCAM applications to process data or to host a low latency service. The register of all the information of the MEC platform in the Discovery service at the 5GMETA Cloud platform. The orchestration and instance type systems to instance pipelines and third-party services on demand. The process is illustrated as follows:","title":"MEC plaform deployement"},{"location":"getting-started-sensor-provider/","text":"Introduction to APIs for sensors and devices The 5GMETA platform exposes a set of APIs for sensor and device developers to register and manage data flows to start sending Data to the platform, these building blocks include Discovery and Registration as described below. Discovery : The building block allows Sensors and Devices developers to discover available MECs and their locations based on their geographical location and tile coverage to be forwarded to the nearest MEC to register your dataflow. Registration : This building block allows developers to register a data flow by providing metadata related to the data flow. The output of this method is a JSON containing the ID of the generated data flow, the topic where to publish, and a Boolean indicating whether or not there is a consumer listening for that type of data. Once a consumer connects to the data registered, the 5GMETA platform will update the Boolean value in order to start producing data to the connected parties. This will trigger a keepalive mechanism to determine which data flows are still active. The output of this method is the same information retrieved at the registration, including the ID of the data flow and its metadata. 5GMETA platform supports three types of data provided by sensors and devices: CITS json like messages with data extracted from the vehicle (ex: GPS position, etc) Example images jpg Video streaming","title":"Getting Started"},{"location":"getting-started-sensor-provider/#introduction-to-apis-for-sensors-and-devices","text":"The 5GMETA platform exposes a set of APIs for sensor and device developers to register and manage data flows to start sending Data to the platform, these building blocks include Discovery and Registration as described below. Discovery : The building block allows Sensors and Devices developers to discover available MECs and their locations based on their geographical location and tile coverage to be forwarded to the nearest MEC to register your dataflow. Registration : This building block allows developers to register a data flow by providing metadata related to the data flow. The output of this method is a JSON containing the ID of the generated data flow, the topic where to publish, and a Boolean indicating whether or not there is a consumer listening for that type of data. Once a consumer connects to the data registered, the 5GMETA platform will update the Boolean value in order to start producing data to the connected parties. This will trigger a keepalive mechanism to determine which data flows are still active. The output of this method is the same information retrieved at the registration, including the ID of the data flow and its metadata. 5GMETA platform supports three types of data provided by sensors and devices: CITS json like messages with data extracted from the vehicle (ex: GPS position, etc) Example images jpg Video streaming","title":"Introduction to APIs for sensors and devices"},{"location":"getting-started/","text":"Introduction to the APIs for third-party CCAM application developpers The 5GMETA platform provides various building blocks that developers can use to develop CCAM applications on top of 5GMETA. These building blocks include Identity, Discovery, Dataflow, Instance Type, License, Data Gateway which can be used as follows: Identity : Developers can use this building block to retrieve the OpenID configuration and request access tokens to authenticate API requests. Discovery : This building block enables developers to browse available MECs, locations, and tile coverage to select the desired locations. Dataflow : With this building block, developers can browse available data flows and datatypes that are available in the desired locations (MEC/ MECs) and select the ones that are relevant to their CCAM application and subscribe to the data flows that are most useful to their application, using the Platform APIs to consume data. Instance Type : 5GMETA platform enables developers to select the preferred instance type and deployment configuration for their CCAM application, determining how much computational power is needed for their application. License : Allows developers to select the preferred license for their CCAM application, ensuring they have the necessary rights to use the data and services provided by the platform. Data Gateway building block, developers can access the Platform APIs and deploy their CCAM application on top of the 5GMETA platform. After all the necessary operations have been performed, the 5GMETA platform processes the request and provides the credentials needed to connect to the platform and Kafka topic to start consuming and building their application on top of 5GMETA. Requirements for interfacing your application 5GMETA platform access To interface your application, you need to have access to a running instance of the 5GMETA platform. During the 5GMETA project lifetime, 5GMETA platform can is available at http://5gmeta-platform.eu . If you want to be granted an access, please contact a platform administrator. Software requirements Ubuntu, preferrably 20.04 python3 Extra packages to be installed Then, you will need to install some dependencies (apt-get): python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad (only if you are going to consume video) gstreamer1.0-libav (only if you are going to consume video) python3-gst-1.0 (only if you are going to consume video) Also install with pip3: kafka-python numpy python-qpid-proton requests confluent-kafka (at least version 2.0.2) avro avro-python3 fastavro avro_to_python Also find easy installation for all the required packages(Be careful of your environment compatibility): pip3 install -r examples/stream-data-gateway/requirements.txt","title":"Getting Started"},{"location":"getting-started/#introduction-to-the-apis-for-third-party-ccam-application-developpers","text":"The 5GMETA platform provides various building blocks that developers can use to develop CCAM applications on top of 5GMETA. These building blocks include Identity, Discovery, Dataflow, Instance Type, License, Data Gateway which can be used as follows: Identity : Developers can use this building block to retrieve the OpenID configuration and request access tokens to authenticate API requests. Discovery : This building block enables developers to browse available MECs, locations, and tile coverage to select the desired locations. Dataflow : With this building block, developers can browse available data flows and datatypes that are available in the desired locations (MEC/ MECs) and select the ones that are relevant to their CCAM application and subscribe to the data flows that are most useful to their application, using the Platform APIs to consume data. Instance Type : 5GMETA platform enables developers to select the preferred instance type and deployment configuration for their CCAM application, determining how much computational power is needed for their application. License : Allows developers to select the preferred license for their CCAM application, ensuring they have the necessary rights to use the data and services provided by the platform. Data Gateway building block, developers can access the Platform APIs and deploy their CCAM application on top of the 5GMETA platform. After all the necessary operations have been performed, the 5GMETA platform processes the request and provides the credentials needed to connect to the platform and Kafka topic to start consuming and building their application on top of 5GMETA.","title":"Introduction to the APIs for third-party CCAM application developpers"},{"location":"getting-started/#requirements-for-interfacing-your-application","text":"","title":"Requirements for interfacing your application"},{"location":"getting-started/#5gmeta-platform-access","text":"To interface your application, you need to have access to a running instance of the 5GMETA platform. During the 5GMETA project lifetime, 5GMETA platform can is available at http://5gmeta-platform.eu . If you want to be granted an access, please contact a platform administrator.","title":"5GMETA platform access"},{"location":"getting-started/#software-requirements","text":"Ubuntu, preferrably 20.04 python3","title":"Software requirements"},{"location":"getting-started/#extra-packages-to-be-installed","text":"Then, you will need to install some dependencies (apt-get): python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad (only if you are going to consume video) gstreamer1.0-libav (only if you are going to consume video) python3-gst-1.0 (only if you are going to consume video) Also install with pip3: kafka-python numpy python-qpid-proton requests confluent-kafka (at least version 2.0.2) avro avro-python3 fastavro avro_to_python Also find easy installation for all the required packages(Be careful of your environment compatibility): pip3 install -r examples/stream-data-gateway/requirements.txt","title":"Extra packages to be installed"},{"location":"image-sender-message-data-broker/","text":"In this page, we introduce examples which can be run for image data type. Steps to run examples The examples can be found in the folder examples/message-data-broker/image_sender_python . Similarly to the C-ITS sender example , you need to configure the address of the data broker. Then, in one terminal window run: python3 sender.py or python3 sender_with_sd_database_support.py Depending if your are running your S&D connected to an database or not You can put the images you are going to send in sample_images folder. If you want to do some debugging and check if your messages are being sent run in another terminal to receive messages (you have to modify address.py in order to put the appropriate ip, port and topic given by your message broker) : python receiver.py Use the ActiveMQ admin web page to check Messages Enqueued / Dequeued counts match. You can control which AMQP server the examples try to connect to and the messages they send by changing the values in config.py You have to take into account that any modification made on dataflowmetadata must be applied too into the content.py file in order to generate the appropriate content. Other resources The initial example for this client is at the link - *https://qpid.apache.org/releases/qpid-proton-0.36.0/proton/python/docs/tutorial.html* - *https://access.redhat.com/documentation/en-us/red_hat_amq/6.3/html/client_connectivity_guide/amqppython*","title":"Sending images guide"},{"location":"image-sender-message-data-broker/#steps-to-run-examples","text":"The examples can be found in the folder examples/message-data-broker/image_sender_python . Similarly to the C-ITS sender example , you need to configure the address of the data broker. Then, in one terminal window run: python3 sender.py or python3 sender_with_sd_database_support.py Depending if your are running your S&D connected to an database or not You can put the images you are going to send in sample_images folder. If you want to do some debugging and check if your messages are being sent run in another terminal to receive messages (you have to modify address.py in order to put the appropriate ip, port and topic given by your message broker) : python receiver.py Use the ActiveMQ admin web page to check Messages Enqueued / Dequeued counts match. You can control which AMQP server the examples try to connect to and the messages they send by changing the values in config.py You have to take into account that any modification made on dataflowmetadata must be applied too into the content.py file in order to generate the appropriate content.","title":"Steps to run examples"},{"location":"image-sender-message-data-broker/#other-resources","text":"The initial example for this client is at the link - *https://qpid.apache.org/releases/qpid-proton-0.36.0/proton/python/docs/tutorial.html* - *https://access.redhat.com/documentation/en-us/red_hat_amq/6.3/html/client_connectivity_guide/amqppython*","title":"Other resources"},{"location":"low-latency-services/","text":"Platform instantiation of Low Latency Data services Third Parties utilizing the 5GMETA Platform can request the deployment of a CCAM application in a MEC Server of the 5GMETA platform. By leveraging the direct access to the 5G network of data producers, these deployments enable swift information retrieval, significantly reducing latency. Additionally, the MEC infrastructure offers optimized edge computing capabilities, enhancing scalability, reliability, and resource utilization. To ensure the security and integrity of the MEC environment, a stringent deployment procedure is followed. The procedure for this deployment works as follows: Contacting the 5GMETA Operator: the Third Party provides essential artifacts, including the Helm chart and the VNF (Virtual Network Function) descriptor, to the 5GMETA Operator. Quarantine Procedure: the 5GMETA Operator deploys the application in a dedicated test environment. Testing Application Behaviour: during this phase, the 5GMETA Operator conducts comprehensive testing to validate the application correct behaviour. These tests verify the proper utilization of the MEC resources and the correct access to the 5GMETA data. Deployment in the MEC environment: once the application passes the tests, it can be deployed on demand by the Third Party, using the same technical procedure used to deploy the modules of the MEC pipelines.","title":"Instanciating low latency data services"},{"location":"low-latency-services/#platform-instantiation-of-low-latency-data-services","text":"Third Parties utilizing the 5GMETA Platform can request the deployment of a CCAM application in a MEC Server of the 5GMETA platform. By leveraging the direct access to the 5G network of data producers, these deployments enable swift information retrieval, significantly reducing latency. Additionally, the MEC infrastructure offers optimized edge computing capabilities, enhancing scalability, reliability, and resource utilization. To ensure the security and integrity of the MEC environment, a stringent deployment procedure is followed. The procedure for this deployment works as follows: Contacting the 5GMETA Operator: the Third Party provides essential artifacts, including the Helm chart and the VNF (Virtual Network Function) descriptor, to the 5GMETA Operator. Quarantine Procedure: the 5GMETA Operator deploys the application in a dedicated test environment. Testing Application Behaviour: during this phase, the 5GMETA Operator conducts comprehensive testing to validate the application correct behaviour. These tests verify the proper utilization of the MEC resources and the correct access to the 5GMETA data. Deployment in the MEC environment: once the application passes the tests, it can be deployed on demand by the Third Party, using the same technical procedure used to deploy the modules of the MEC pipelines.","title":"Platform instantiation of Low Latency Data services"},{"location":"message-databroker/","text":"Message Data Broker Description Message Data Broker is the module used in 5GMETA platform to stream the messages from Sensors&Devices to the 5G MECs (Multi-access edge computing). Installation Installation requirements can be found in the repository. Clone the repository using git clone https://github.com/5gmeta/message-data-broker.git Table of Contents This repository contains the following modules: An ActiveMQ Message Broker ( link ) - which is deployed in the MEC Examples of sender and receiver in python Note: Detailed instructions to build the ActiveMQ source code are available here . Development instructions for Message Data Broker Message Broker can be found in the /src folder Steps to run: sudo docker-compose up -d open http:// :8161 manage ActiveMQ broker admin/admin you can see created topics Required packages pip dependencies python-qpid-proton Examples In the /examples/activemq_clients you will find sample code for different types of AMQP producers as follows: cits_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicles that send messages with some properties attached. image_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicle that sends images (different size, but all the same) cits_receiver_python : a python example to receive messages from AMQP events These producer/consumer examples are described in detail in the following section.","title":"Message data broker"},{"location":"message-databroker/#message-data-broker","text":"","title":"Message Data Broker"},{"location":"message-databroker/#description","text":"Message Data Broker is the module used in 5GMETA platform to stream the messages from Sensors&Devices to the 5G MECs (Multi-access edge computing).","title":"Description"},{"location":"message-databroker/#installation","text":"Installation requirements can be found in the repository. Clone the repository using git clone https://github.com/5gmeta/message-data-broker.git","title":"Installation"},{"location":"message-databroker/#table-of-contents","text":"This repository contains the following modules: An ActiveMQ Message Broker ( link ) - which is deployed in the MEC Examples of sender and receiver in python Note: Detailed instructions to build the ActiveMQ source code are available here .","title":"Table of Contents"},{"location":"message-databroker/#development-instructions-for-message-data-broker","text":"Message Broker can be found in the /src folder Steps to run: sudo docker-compose up -d open http:// :8161 manage ActiveMQ broker admin/admin you can see created topics","title":"Development instructions for Message Data Broker"},{"location":"message-databroker/#required-packages","text":"pip dependencies python-qpid-proton","title":"Required packages"},{"location":"message-databroker/#examples","text":"In the /examples/activemq_clients you will find sample code for different types of AMQP producers as follows: cits_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicles that send messages with some properties attached. image_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicle that sends images (different size, but all the same) cits_receiver_python : a python example to receive messages from AMQP events These producer/consumer examples are described in detail in the following section.","title":"Examples"},{"location":"processing_description/","text":"Advanced description of 5GMETA platform concepts Introduction This documentation introduces the APIs needed to be able to produce and consume various types of user data to the 5GMETA platform. Background 5GMETA Platform Framework The 5GMETA platform can be seen as a meeting-point between CAM data producers and consumers. It allows the data producers to post data from their data sources (e.g. sensors, camera) so that the data consumers can consume them in streaming mode in order to create data-driven innovative services for their clients. Streaming mode means that the 5GMETA platform doesn\u2019t store data. The data flow from the data producers called Sensors and Devices, through the 5GMETA platform and towards the data consumers which are CAM Services and Applications that rely on CAM data. The Sensors and Devices (S&D) represent the data producers with their data sources generating the data that are sent to the platform. The data sources can be vehicle\u2019s sensors, cameras etc. They communicate with the 5GMETA platform using 5G network. The CAM Services and Applications act as data consumers accessing the data that are made available through the platform. The 5GMETA MEC acts as an intermediate between the Sensors and Devices and the cloud layer. It can be seen as a part of the 5GMETA platform that is deployed close to where the data are produced. Its purpose it to serve the data producers in its vicinity. APIs Description As seen in the high-level platform architecture above, following are some APIs in the Cloud and the MEC layers which are important to know for producing and consuming data. Cloud layer APIs: Discovery API : This API allows MEC infrastructures to publish the credentials and metadata including endpoints, coverage and inventories available computing resources to host data pipelines, indexing all the 5GMETA MEC infrastructures data services, the information to be operated by the cloud, according to third-party application/service developer requests, and the configuration to connect to and from Sensors and Devices. Stream Data : This API makes connections on demand between the cloud platform and the selected MEC platforms from where capture the data according to third-party application/service developer requests, and push all the applicable data samples into the individual topic for the third-party application/service. Dataflow : The Dataflow API stores the request of any third-party application/service developer request in terms of data and geographic production tiles to trigger through the Registration API the data sending from Sensors and Devices, and to create the individual topic for the third-party application/service. The Data Gateway API : This provides to a third-party application/service the endpoint and the topic to produce or consume data limiting the access to legitimate and valid third-party systems. MEC layer APIs: Registration API : The Registration API is a RESTful application working as main interface of communication with the Sensors & Devices. The S&D should connect to this component before publishing messages to receive the DataflowId and Topic needed to publish data using the internal Message Broker. This component also should receive a Keep Alive from the S&D: if the S&D stops sending the Keep alive, the APIs remove its dataflow from the available ones in the platform. The Message Broker : The Message Broker a broker implementing the publish-subscribe pattern. This broker is then connected to the Stream Data in the cloud side of the platform, both to upload data from the S&D to the CAM Services and Applications and to send events from the CAM Services and Applications to the S&D. Video Broker : The Video Broker is a Proxy for video streams coming from the Sensors and Devices. The supported protocols for the streams are V4L, RTP and RTSP. Sensor and Device Interfaces This section describes different aspects on how Sensors & Devices operate and interact with the 5GMETA platform to connect different data sources with a specific production application or service. The communication interfaces described in this section allow the Sensors & Devices to produce data to the platform and receive events from the Third Parties that are consuming data in the platform. A publish-subscribe protocol : the Edge Message Broker implements the publish-subscribe paradigm, so to send messages in the platform is mandatory to support the specific protocols supported by this component (such as MQTT or AMQP). Streaming Standards : Sensors and Devices can produce media streams in RTSP, UDP or WebRTC formats. The data APIs will provide access to WebRTC media streams to CCAM applications. HTTP : the Registration API and the Discovery API in the Edge provide services as a RESTful application, so HTTP can be used to interface with those 5GMETA components is by means of HTTP requests. Data Production Flow The Data Production allows any S&D to publish data in the platform, to be then consumed by the Third Parties. In order to do that, S&Ds can connect to the APIs exposed by the platform to define and register the dataflows it wants to produce and which MEC infrastructure to connect to depending on their geolocation. This interaction is summarized in figure shown below. Event Consumption Flow The Event Consumption allows S&D to receive messages that were generated by CAM Services and Applications. To do so, the S&D has to subscribe to a specific Topic in the Edge Broker, connected to the Stream of data in which the CAM Services and Applications are publishing the events. This interaction is summarized in figure below. CAM Services and Applications Interfaces The communication interfaces described in this section allow to the CAM Services and Applications to register to the platform, consume data from the platform and generate events that will be received from the Sensors & Devices. A publish-subscribe protocol : the Stream Data implements the publish-subscribe paradigm, so to receive messages from the platform is mandatory to support the specific protocols supported by this component (such as KAFKA). HTTP : The Dataflow API, Identity API and cloud Instance API provide services in the form of a RESTful application, so the only way to interface with those 5GMETA components is by means of HTTP requests. WebRTC : Video data from the 5GMETA platform will be sent through the WebRTC protocol. Data Consumption Procedure The Data Consumption allows a CAM Service and Application to receive data from any of the Sensors and Devices publishing in the platform. To do so, a CAM Service and Application can connect to the APIs exposed by the platform to define the dataflows it wants to consume and which pre-processing pipeline. This interaction is summarized in below. Event Generation Procedure The Event Generation allows a CAM Service and Application to generate events and send them to the Sensors and Devices connected to the platform. To do so, a CAM Service and Application connects to the APIs exposed by the platform to generate messages that will be forwarded through the platform to the Sensors and Devices. This interaction is summarized in the figure below.","title":"Advcanced description"},{"location":"processing_description/#advanced-description-of-5gmeta-platform-concepts","text":"","title":"Advanced description of 5GMETA platform concepts"},{"location":"processing_description/#introduction","text":"This documentation introduces the APIs needed to be able to produce and consume various types of user data to the 5GMETA platform.","title":"Introduction"},{"location":"processing_description/#background","text":"","title":"Background"},{"location":"processing_description/#5gmeta-platform-framework","text":"The 5GMETA platform can be seen as a meeting-point between CAM data producers and consumers. It allows the data producers to post data from their data sources (e.g. sensors, camera) so that the data consumers can consume them in streaming mode in order to create data-driven innovative services for their clients. Streaming mode means that the 5GMETA platform doesn\u2019t store data. The data flow from the data producers called Sensors and Devices, through the 5GMETA platform and towards the data consumers which are CAM Services and Applications that rely on CAM data. The Sensors and Devices (S&D) represent the data producers with their data sources generating the data that are sent to the platform. The data sources can be vehicle\u2019s sensors, cameras etc. They communicate with the 5GMETA platform using 5G network. The CAM Services and Applications act as data consumers accessing the data that are made available through the platform. The 5GMETA MEC acts as an intermediate between the Sensors and Devices and the cloud layer. It can be seen as a part of the 5GMETA platform that is deployed close to where the data are produced. Its purpose it to serve the data producers in its vicinity.","title":"5GMETA Platform Framework"},{"location":"processing_description/#apis-description","text":"As seen in the high-level platform architecture above, following are some APIs in the Cloud and the MEC layers which are important to know for producing and consuming data. Cloud layer APIs: Discovery API : This API allows MEC infrastructures to publish the credentials and metadata including endpoints, coverage and inventories available computing resources to host data pipelines, indexing all the 5GMETA MEC infrastructures data services, the information to be operated by the cloud, according to third-party application/service developer requests, and the configuration to connect to and from Sensors and Devices. Stream Data : This API makes connections on demand between the cloud platform and the selected MEC platforms from where capture the data according to third-party application/service developer requests, and push all the applicable data samples into the individual topic for the third-party application/service. Dataflow : The Dataflow API stores the request of any third-party application/service developer request in terms of data and geographic production tiles to trigger through the Registration API the data sending from Sensors and Devices, and to create the individual topic for the third-party application/service. The Data Gateway API : This provides to a third-party application/service the endpoint and the topic to produce or consume data limiting the access to legitimate and valid third-party systems. MEC layer APIs: Registration API : The Registration API is a RESTful application working as main interface of communication with the Sensors & Devices. The S&D should connect to this component before publishing messages to receive the DataflowId and Topic needed to publish data using the internal Message Broker. This component also should receive a Keep Alive from the S&D: if the S&D stops sending the Keep alive, the APIs remove its dataflow from the available ones in the platform. The Message Broker : The Message Broker a broker implementing the publish-subscribe pattern. This broker is then connected to the Stream Data in the cloud side of the platform, both to upload data from the S&D to the CAM Services and Applications and to send events from the CAM Services and Applications to the S&D. Video Broker : The Video Broker is a Proxy for video streams coming from the Sensors and Devices. The supported protocols for the streams are V4L, RTP and RTSP.","title":"APIs Description"},{"location":"processing_description/#sensor-and-device-interfaces","text":"This section describes different aspects on how Sensors & Devices operate and interact with the 5GMETA platform to connect different data sources with a specific production application or service. The communication interfaces described in this section allow the Sensors & Devices to produce data to the platform and receive events from the Third Parties that are consuming data in the platform. A publish-subscribe protocol : the Edge Message Broker implements the publish-subscribe paradigm, so to send messages in the platform is mandatory to support the specific protocols supported by this component (such as MQTT or AMQP). Streaming Standards : Sensors and Devices can produce media streams in RTSP, UDP or WebRTC formats. The data APIs will provide access to WebRTC media streams to CCAM applications. HTTP : the Registration API and the Discovery API in the Edge provide services as a RESTful application, so HTTP can be used to interface with those 5GMETA components is by means of HTTP requests.","title":"Sensor and Device Interfaces"},{"location":"processing_description/#data-production-flow","text":"The Data Production allows any S&D to publish data in the platform, to be then consumed by the Third Parties. In order to do that, S&Ds can connect to the APIs exposed by the platform to define and register the dataflows it wants to produce and which MEC infrastructure to connect to depending on their geolocation. This interaction is summarized in figure shown below.","title":"Data Production Flow"},{"location":"processing_description/#event-consumption-flow","text":"The Event Consumption allows S&D to receive messages that were generated by CAM Services and Applications. To do so, the S&D has to subscribe to a specific Topic in the Edge Broker, connected to the Stream of data in which the CAM Services and Applications are publishing the events. This interaction is summarized in figure below.","title":"Event Consumption Flow"},{"location":"processing_description/#cam-services-and-applications-interfaces","text":"The communication interfaces described in this section allow to the CAM Services and Applications to register to the platform, consume data from the platform and generate events that will be received from the Sensors & Devices. A publish-subscribe protocol : the Stream Data implements the publish-subscribe paradigm, so to receive messages from the platform is mandatory to support the specific protocols supported by this component (such as KAFKA). HTTP : The Dataflow API, Identity API and cloud Instance API provide services in the form of a RESTful application, so the only way to interface with those 5GMETA components is by means of HTTP requests. WebRTC : Video data from the 5GMETA platform will be sent through the WebRTC protocol.","title":"CAM Services and Applications Interfaces"},{"location":"processing_description/#data-consumption-procedure","text":"The Data Consumption allows a CAM Service and Application to receive data from any of the Sensors and Devices publishing in the platform. To do so, a CAM Service and Application can connect to the APIs exposed by the platform to define the dataflows it wants to consume and which pre-processing pipeline. This interaction is summarized in below.","title":"Data Consumption Procedure"},{"location":"processing_description/#event-generation-procedure","text":"The Event Generation allows a CAM Service and Application to generate events and send them to the Sensors and Devices connected to the platform. To do so, a CAM Service and Application connects to the APIs exposed by the platform to generate messages that will be forwarded through the platform to the Sensors and Devices. This interaction is summarized in the figure below.","title":"Event Generation Procedure"},{"location":"producing_events_guide/","text":"Guide to produce an event Platform-client helper application There is a guided applicaction that will help you to get the apropriate parameters from 5GMETA platform to get the data you need. You can execute it by downloading all content from folder: Platform client helper application Once you have donwload that software you can run it by executing: $ python3 client.py in your command line. Client usage Once you have executed the previous command you will be prompted for: 5GMETA username 5GMETA password Produce or Consume Tile After entering your username/password, client will ask you if you want to: - Consume data from 5GMETA platform - Produce an event in a vehicle connected to 5GMETA platform In our case we have decided to produce an event, so we push e. Inmediately client will prompt you which tile to push an event to. The Client will prompt out a topic where you can push an event, copy paste it inside the config file Execute in your terminal indicating the MESSAGE (EVENT) you want to push to 5GMETA platform as follow: $ python3 kafka-even-sender.py MESSAGE It will prompt you with the success of sending your event to 5GMETA platform with your indicated TOPIC from the client.","title":"Producing Events Guide"},{"location":"producing_events_guide/#guide-to-produce-an-event","text":"","title":"Guide to produce an event"},{"location":"producing_events_guide/#platform-client-helper-application","text":"There is a guided applicaction that will help you to get the apropriate parameters from 5GMETA platform to get the data you need. You can execute it by downloading all content from folder: Platform client helper application Once you have donwload that software you can run it by executing: $ python3 client.py in your command line.","title":"Platform-client helper application"},{"location":"producing_events_guide/#client-usage","text":"Once you have executed the previous command you will be prompted for: 5GMETA username 5GMETA password Produce or Consume Tile After entering your username/password, client will ask you if you want to: - Consume data from 5GMETA platform - Produce an event in a vehicle connected to 5GMETA platform In our case we have decided to produce an event, so we push e. Inmediately client will prompt you which tile to push an event to. The Client will prompt out a topic where you can push an event, copy paste it inside the config file Execute in your terminal indicating the MESSAGE (EVENT) you want to push to 5GMETA platform as follow: $ python3 kafka-even-sender.py MESSAGE It will prompt you with the success of sending your event to 5GMETA platform with your indicated TOPIC from the client.","title":"Client usage"},{"location":"receiving_events_guide/","text":"This is an example of how to receive event from 5GMETA platform using reactor API with ActiveMQ. Running the Examples The examples can be found in the folder examples/message-data-broker/events_receiver_python . In one terminal window run: python3 receiver_events.py","title":"Receving Events guide"},{"location":"receiving_events_guide/#running-the-examples","text":"The examples can be found in the folder examples/message-data-broker/events_receiver_python . In one terminal window run: python3 receiver_events.py","title":"Running the Examples"},{"location":"stream-datagateway/","text":"Stream Data Gateway Description Stream Data Gateway is the module used in 5GMETA Cloud platform to stream the messages from the MECs to the Third Parties. It is implemented using Kafka ecosystem technologies. This repository can be downloaded from here . Table of Contents The Stream Data Gatawey module contains: Kafka Broker docker-compose and helm chart ( link ). Connectors configuration to retrive the messages from ActiveMQ (src/) Examples of different Kafka Consumers ( link ) Development version for Stream Data Gateway Refer this README for further API details. The dev version can be found here . There is Kafka instance to be deployed locally. Inside src/dev-version/connectors there is CLI to create connectors between AMQP (MEC) and Kafka (Cloud) to push data into Kafka infrastructure. Production version Contains all neccessary informations to deploy Kafka into an AWS infrastructure ( link ). 5GMETA Platform client This is a third party python client for making requests to 5GMETA Platform. Clone the contents of platform-client . Prerequisites Python3 Registered user in the platform. To register use: 5GMETA User Registration Examples In the /examples folder you can find different sample codes for different Kafka producers and consumers as follows: Consumer: Kafka Consumers: working with AVRO (serialization and deserialization). /consumer/cits/cits-consumer.py : a sample python Kafka consumer to receive CITS messages. /consumer/image/image-consumer.py : a sample python Kafka consumer to receive images. /consumer/video/video-consumer.py : a sample python Kafka consumer to receive video streams. Producer: A Kafka producer ( avro_producer_events.py ) as example for the Third Parties message producer. Other: An interactive Avro CLI client to play with Kafka with AVRO ser-deser. Other example of producers Kafka Event message sender Installation Installation requirements can be found on producer/consumer folders respectively. - Clone the repository using git clone https://github.com/5gmeta/stream-data-gateway.git Navigate to /examples folder to install dependencies using: pip install -r examples/requirements.txt Kafka producer/consumer examples are described in further detail in the following section.","title":"Stream data gateway"},{"location":"stream-datagateway/#stream-data-gateway","text":"","title":"Stream Data Gateway"},{"location":"stream-datagateway/#description","text":"Stream Data Gateway is the module used in 5GMETA Cloud platform to stream the messages from the MECs to the Third Parties. It is implemented using Kafka ecosystem technologies. This repository can be downloaded from here .","title":"Description"},{"location":"stream-datagateway/#table-of-contents","text":"The Stream Data Gatawey module contains: Kafka Broker docker-compose and helm chart ( link ). Connectors configuration to retrive the messages from ActiveMQ (src/) Examples of different Kafka Consumers ( link )","title":"Table of Contents"},{"location":"stream-datagateway/#development-version-for-stream-data-gateway","text":"Refer this README for further API details. The dev version can be found here . There is Kafka instance to be deployed locally. Inside src/dev-version/connectors there is CLI to create connectors between AMQP (MEC) and Kafka (Cloud) to push data into Kafka infrastructure.","title":"Development version for Stream Data Gateway"},{"location":"stream-datagateway/#production-version","text":"Contains all neccessary informations to deploy Kafka into an AWS infrastructure ( link ).","title":"Production version"},{"location":"stream-datagateway/#5gmeta-platform-client","text":"This is a third party python client for making requests to 5GMETA Platform. Clone the contents of platform-client .","title":"5GMETA Platform client"},{"location":"stream-datagateway/#prerequisites","text":"Python3 Registered user in the platform. To register use: 5GMETA User Registration","title":"Prerequisites"},{"location":"stream-datagateway/#examples","text":"In the /examples folder you can find different sample codes for different Kafka producers and consumers as follows:","title":"Examples"},{"location":"stream-datagateway/#consumer","text":"Kafka Consumers: working with AVRO (serialization and deserialization). /consumer/cits/cits-consumer.py : a sample python Kafka consumer to receive CITS messages. /consumer/image/image-consumer.py : a sample python Kafka consumer to receive images. /consumer/video/video-consumer.py : a sample python Kafka consumer to receive video streams.","title":"Consumer:"},{"location":"stream-datagateway/#producer","text":"A Kafka producer ( avro_producer_events.py ) as example for the Third Parties message producer.","title":"Producer:"},{"location":"stream-datagateway/#other","text":"An interactive Avro CLI client to play with Kafka with AVRO ser-deser. Other example of producers Kafka Event message sender","title":"Other:"},{"location":"stream-datagateway/#installation","text":"Installation requirements can be found on producer/consumer folders respectively. - Clone the repository using git clone https://github.com/5gmeta/stream-data-gateway.git Navigate to /examples folder to install dependencies using: pip install -r examples/requirements.txt Kafka producer/consumer examples are described in further detail in the following section.","title":"Installation"},{"location":"stream-example/","text":"Consumer and Event Producer Examples In this section, we walkthrough the Kafka consumer examples. Three sample consumer examples are provided within the repo for cits, video and image datatypes respectively. Usage Consumer instructions Use platform-client to receive appropriate topics and IPs and ports to be used. Select the suitable consumer as per the produced data and use as follows: python3 cits-consumer.py topic platformaddress bootstrap_port registry_port or python3 video-consumer.py platformaddress bootstrap_port topic dataflow_id Avro events producer examples This folder contains avro producer example to produce events. Usage Avro Producer config format to produce an event with avro serializer: schema_registry_client = SchemaRegistryClient(schema_registry_conf) avro_serializer = AvroSerializer( schema_registry_client, schema_str, msg_to_dict ) producer_conf = {'bootstrap.servers': args.bootstrap_servers, 'key.serializer': StringSerializer('utf_8'), 'value.serializer': avro_serializer} producer = SerializingProducer(producer_conf) . . . producer.poll(0.0) Usage is: python3 avro_producer_events.py -b bootstrap_ip:9092 -s http://schema_ip:8081 -t topic More examples There are other such examples that are complete and don't need to use external util to get topic and ip/port to access the system. cits/cits-kafka-consumer.py image/image-kafka-consumer.py","title":"Additional examples"},{"location":"stream-example/#consumer-and-event-producer-examples","text":"In this section, we walkthrough the Kafka consumer examples. Three sample consumer examples are provided within the repo for cits, video and image datatypes respectively.","title":"Consumer and Event Producer Examples"},{"location":"stream-example/#usage","text":"","title":"Usage"},{"location":"stream-example/#consumer-instructions","text":"Use platform-client to receive appropriate topics and IPs and ports to be used. Select the suitable consumer as per the produced data and use as follows: python3 cits-consumer.py topic platformaddress bootstrap_port registry_port or python3 video-consumer.py platformaddress bootstrap_port topic dataflow_id","title":"Consumer instructions"},{"location":"stream-example/#avro-events-producer-examples","text":"This folder contains avro producer example to produce events.","title":"Avro events producer examples"},{"location":"stream-example/#usage_1","text":"Avro Producer config format to produce an event with avro serializer: schema_registry_client = SchemaRegistryClient(schema_registry_conf) avro_serializer = AvroSerializer( schema_registry_client, schema_str, msg_to_dict ) producer_conf = {'bootstrap.servers': args.bootstrap_servers, 'key.serializer': StringSerializer('utf_8'), 'value.serializer': avro_serializer} producer = SerializingProducer(producer_conf) . . . producer.poll(0.0) Usage is: python3 avro_producer_events.py -b bootstrap_ip:9092 -s http://schema_ip:8081 -t topic","title":"Usage"},{"location":"stream-example/#more-examples","text":"There are other such examples that are complete and don't need to use external util to get topic and ip/port to access the system. cits/cits-kafka-consumer.py image/image-kafka-consumer.py","title":"More examples"},{"location":"tutorial-processing/","text":"Tutorial for processing data with 5GMETA platform This page introduces basic example for the processing of data gathered from 5GMETA platform. These tutorials can be found in examples/tutorials directory. Requirements python3 matplotlib (pip install matplotlib) folium (pip install folium) Also find easy installation for all the required packages with requirements.txt file (Be careful of your environment compatibility): pip3 install -r examples/tutorials/requirements.txt Example 1: Parallel processing of data consumed In this example, we show how data can be consumed in a dedicated thread and a specific value (here, the speed of a vehicle) can be displayed in a GUI First, you need to run the client platform (client_example1.py) to obtain the access to the data python client_example1.py Upon the request, enter your credentials: (username, password) Once you have successfully registered, you can obtain a topic from which you can consume data. Please to not close the platform client script. Then, open a new terminal and run the script example1_oscilloscope.py using the command prompted in the platform client python example1_oscilloscope.py topic platformaddress bootstrap_port registry_port You will start receive data and extract speed value to show in live in a GUI Example 2: Collection of location data with map display In this example, we show how data can be collected from a given topic and location information is extracted and displayed using openstreetmap First, you need to run the client platform (client_example2.py) to obtain the access to the data python client_example2.py Upon the request, enter your credentials: (username, password) Once you have successfully registered, you can obtain a topic from which you can consume data. Please to not close the platform client script. Then, open a new terminal and run the script example2_displaymap.py using the command prompted in the platform client python example2_displaymap.py topic platformaddress bootstrap_port registry_port You will start collecting a number of samples (e.g. 20) and store the location values in a dedicated table. At the end of the processing, a map file is generated to display the received location values. You can load the map file, 5GMETADataMap,html, in your browser to show the location values as illustrated in the figure below. Example 3: Produce a feedback event from consumption of data In this example, we show how a feedback event can be generated to a specific tile after the processing of data consumed from 5GMETA platform. First, you need to run the client platform (client_example3.py) to obtain the access to the data python client_example3.py Upon the request, enter your credentials: (username, password) Once you have successfully registered, you will obtain the access to 2 topics: 1 for the consumption of data and 1 for the generation of an event feedback. Please to not close the platform client script. Then, open a new terminal and run the script example3_eventfeedback.py using the command prompted in the platform client python example3_eventfeedback.py consume_topic produce_topic platformaddress bootstrap_port registry_port You will start consuming data using the consume topic (e.g. 5). After the consumption of a predefined number of message, an event is produced on the 5GMETA platform as illustrated in the figure below.","title":"Data Processing Tutorials"},{"location":"tutorial-processing/#tutorial-for-processing-data-with-5gmeta-platform","text":"This page introduces basic example for the processing of data gathered from 5GMETA platform. These tutorials can be found in examples/tutorials directory.","title":"Tutorial for processing data with 5GMETA platform"},{"location":"tutorial-processing/#requirements","text":"python3 matplotlib (pip install matplotlib) folium (pip install folium) Also find easy installation for all the required packages with requirements.txt file (Be careful of your environment compatibility): pip3 install -r examples/tutorials/requirements.txt","title":"Requirements"},{"location":"tutorial-processing/#example-1-parallel-processing-of-data-consumed","text":"In this example, we show how data can be consumed in a dedicated thread and a specific value (here, the speed of a vehicle) can be displayed in a GUI First, you need to run the client platform (client_example1.py) to obtain the access to the data python client_example1.py Upon the request, enter your credentials: (username, password) Once you have successfully registered, you can obtain a topic from which you can consume data. Please to not close the platform client script. Then, open a new terminal and run the script example1_oscilloscope.py using the command prompted in the platform client python example1_oscilloscope.py topic platformaddress bootstrap_port registry_port You will start receive data and extract speed value to show in live in a GUI","title":"Example 1: Parallel processing of data consumed"},{"location":"tutorial-processing/#example-2-collection-of-location-data-with-map-display","text":"In this example, we show how data can be collected from a given topic and location information is extracted and displayed using openstreetmap First, you need to run the client platform (client_example2.py) to obtain the access to the data python client_example2.py Upon the request, enter your credentials: (username, password) Once you have successfully registered, you can obtain a topic from which you can consume data. Please to not close the platform client script. Then, open a new terminal and run the script example2_displaymap.py using the command prompted in the platform client python example2_displaymap.py topic platformaddress bootstrap_port registry_port You will start collecting a number of samples (e.g. 20) and store the location values in a dedicated table. At the end of the processing, a map file is generated to display the received location values. You can load the map file, 5GMETADataMap,html, in your browser to show the location values as illustrated in the figure below.","title":"Example 2: Collection of location data with map display"},{"location":"tutorial-processing/#example-3-produce-a-feedback-event-from-consumption-of-data","text":"In this example, we show how a feedback event can be generated to a specific tile after the processing of data consumed from 5GMETA platform. First, you need to run the client platform (client_example3.py) to obtain the access to the data python client_example3.py Upon the request, enter your credentials: (username, password) Once you have successfully registered, you will obtain the access to 2 topics: 1 for the consumption of data and 1 for the generation of an event feedback. Please to not close the platform client script. Then, open a new terminal and run the script example3_eventfeedback.py using the command prompted in the platform client python example3_eventfeedback.py consume_topic produce_topic platformaddress bootstrap_port registry_port You will start consuming data using the consume topic (e.g. 5). After the consumption of a predefined number of message, an event is produced on the 5GMETA platform as illustrated in the figure below.","title":"Example 3: Produce a feedback event from consumption of data"},{"location":"video-stream-broker/","text":"In this page, we introduce examples, including scenario, pre-conditions and scripts/Dockers, to produce video streams. The examples presented are located in the examples/video-stream-broker folder. Scenario A video sensor wants to push a UDP video stream including a RTP video with H264 Pre-Conditions The following systems must be configured and running: The Message Broker (AMQP) is up and running The Kakfa Broker (KAFKA) is up and running and ready at EKS. It polls data through Kafka Connector and KSQLDB from AMQP The GPS positions in examples/video-stream-broker/video_sensor.py to get the tiles are hardcoded in the example removing the integration with a specific GPS device. The component for the Video Sensor is built but not running Usage To Push a UDP Source to 5GMETA MEC First, we need a UDP SOURCE or RTSP Stream with an H.264 video $ gst-launch-1.0 videotestsrc is-live=true do-timestamp=true ! videoconvert ! videorate ! video/x-raw, width=640, height=480, framerate=10/1 ! textoverlay font-desc=\"Arial 40px\" text=\"container TX\" valignment=2 ! timeoverlay font-desc=\"Arial 60px\" valignment=2 ! videoconvert ! tee name=t ! queue max-size-buffers=1 ! x264enc bitrate=2000 speed-preset=ultrafast tune=zerolatency key-int-max=5 ! video/x-h264,profile=constrained-baseline,stream-format=byte-stream ! h264parse ! rtph264pay pt=96 config-interval=-1 name=payloader ! application/x-rtp,media=video,encoding-name=H264,payload=96 ! udpsink host=127.0.0.1 port=7000 sync=false enable-last-sample=false send-duplicates=false Second, we need to run the Video Sensor Docker sending the UDP video stream $ docker run --net=host --env AMQP_USER=\"<user>\" --env AMQP_PASS=\"<password>\" --env VIDEO_SOURCE=\"udp\" --env VIDEO_PARAM=\"7000\" --env VIDEO_TTL=\"100\" 5gmeta/video_sensor Remember to configure the <user >, <password > to your local environment NB: Note that *7000 is the UDP source Port *** Note that *100 is the timeout until the Video Sensor will stop sending a video stream*** Third, we can play the video from the 5GMETA MEC infrastructure, produced by the WebRTC Proxy, allowing data pipelines at the 5GMETA MEC infrastructure to process $ gst-launch-1.0 udpsrc port=5045 ! application/x-rtp,encoding-name=H264,payload=96 ! rtpjitterbuffer latency=50 ! rtph264depay ! decodebin ! videoconvert ! textoverlay font-desc=\"Arial 40px\" text=\"local RX\" valignment=1 ! videoconvert ! autovideosink NB: Note that *5045 is the UDP Port produced by the WebRTC proxy based on the provided ID (from the registration process)*** Local Consume a AMQP Source First, change amqp2video.py code to set the target ID if (event.message.properties['sourceId'] == 45): NB: Note that *45 should be changed to the dataflow ID identified in the logs coming from registration and WebRTC proxy logs*** Second, launch the player $ AMQP_USER=\"<user>\" AMQP_PASS=\"<password>\" AMQP_IP=\"<AAA.BBB.CCC.DDD>\" AMQP_PORT=\"<port>\" GST_DEBUG=3 python3 ./amqp2video.py Remember to configure the <user >, <password >, <AAA.BBB.CCC.DDD > and <port > to your local environment","title":"Sending video streams guide"},{"location":"video-stream-broker/#scenario","text":"A video sensor wants to push a UDP video stream including a RTP video with H264","title":"Scenario"},{"location":"video-stream-broker/#pre-conditions","text":"The following systems must be configured and running: The Message Broker (AMQP) is up and running The Kakfa Broker (KAFKA) is up and running and ready at EKS. It polls data through Kafka Connector and KSQLDB from AMQP The GPS positions in examples/video-stream-broker/video_sensor.py to get the tiles are hardcoded in the example removing the integration with a specific GPS device. The component for the Video Sensor is built but not running","title":"Pre-Conditions"},{"location":"video-stream-broker/#usage","text":"To Push a UDP Source to 5GMETA MEC First, we need a UDP SOURCE or RTSP Stream with an H.264 video $ gst-launch-1.0 videotestsrc is-live=true do-timestamp=true ! videoconvert ! videorate ! video/x-raw, width=640, height=480, framerate=10/1 ! textoverlay font-desc=\"Arial 40px\" text=\"container TX\" valignment=2 ! timeoverlay font-desc=\"Arial 60px\" valignment=2 ! videoconvert ! tee name=t ! queue max-size-buffers=1 ! x264enc bitrate=2000 speed-preset=ultrafast tune=zerolatency key-int-max=5 ! video/x-h264,profile=constrained-baseline,stream-format=byte-stream ! h264parse ! rtph264pay pt=96 config-interval=-1 name=payloader ! application/x-rtp,media=video,encoding-name=H264,payload=96 ! udpsink host=127.0.0.1 port=7000 sync=false enable-last-sample=false send-duplicates=false Second, we need to run the Video Sensor Docker sending the UDP video stream $ docker run --net=host --env AMQP_USER=\"<user>\" --env AMQP_PASS=\"<password>\" --env VIDEO_SOURCE=\"udp\" --env VIDEO_PARAM=\"7000\" --env VIDEO_TTL=\"100\" 5gmeta/video_sensor Remember to configure the <user >, <password > to your local environment NB: Note that *7000 is the UDP source Port *** Note that *100 is the timeout until the Video Sensor will stop sending a video stream*** Third, we can play the video from the 5GMETA MEC infrastructure, produced by the WebRTC Proxy, allowing data pipelines at the 5GMETA MEC infrastructure to process $ gst-launch-1.0 udpsrc port=5045 ! application/x-rtp,encoding-name=H264,payload=96 ! rtpjitterbuffer latency=50 ! rtph264depay ! decodebin ! videoconvert ! textoverlay font-desc=\"Arial 40px\" text=\"local RX\" valignment=1 ! videoconvert ! autovideosink NB: Note that *5045 is the UDP Port produced by the WebRTC proxy based on the provided ID (from the registration process)***","title":"Usage"},{"location":"video-stream-broker/#local-consume-a-amqp-source","text":"First, change amqp2video.py code to set the target ID if (event.message.properties['sourceId'] == 45): NB: Note that *45 should be changed to the dataflow ID identified in the logs coming from registration and WebRTC proxy logs*** Second, launch the player $ AMQP_USER=\"<user>\" AMQP_PASS=\"<password>\" AMQP_IP=\"<AAA.BBB.CCC.DDD>\" AMQP_PORT=\"<port>\" GST_DEBUG=3 python3 ./amqp2video.py Remember to configure the <user >, <password >, <AAA.BBB.CCC.DDD > and <port > to your local environment","title":"Local Consume a AMQP Source"},{"location":"datasets-description/unimore-cam-description/","text":"CAM Dataset The CAM dataset contains messages with information captured by Cooperative Awareness Messages (CAM). Header protocolVersion : The version of the protocol used messageID : A unique ID for the message stationID : The ID of the station that captured the CAM CAM generationDeltaTime : Time since last CAM message basicContainer stationType : The type of the station that captured the CAM referencePosition : latitude : The latitude of the station longitude : The longitude of the station positionConfidenceEllipse : Confidence ellipse of the position semiMajorConfidence : Major axis confidence semiMinorConfidence : Minor axis confidence semiMajorOrientation : Major axis orientation altitude : altitudeValue : The altitude value altitudeConfidence : Confidence in the altitude highFrequencyContainer heading : headingValue : The heading value headingConfidence : Confidence in the heading value speed : speedValue : The speed value speedConfidence : Confidence in the speed value driveDirection : The drive direction (forward, backward,unavailable) vehicleLength : vehicleLengthValue : The length of the vehicle vehicleLengthConfidenceIndication : Confidence in the vehicle length vehicleWidth : The width of the vehicle longitudinalAcceleration : longitudinalAccelerationValue : The longitudinal acceleration value longitudinalAccelerationConfidence : Confidence in the longitudinal acceleration curvature : curvatureValue : The curvature value curvatureConfidence : Confidence in the curvature value curvatureCalculationMode : How the curvature was calculated yawRate : yawRateValue : The yaw rate value yawRateConfidence : Confidence in the yaw rate value accelerationControl : Acceleration control status lanePosition : The lane position of the vehicle","title":"CAM Dataset"},{"location":"datasets-description/unimore-cam-description/#cam-dataset","text":"The CAM dataset contains messages with information captured by Cooperative Awareness Messages (CAM).","title":"CAM Dataset"},{"location":"datasets-description/unimore-cam-description/#header","text":"protocolVersion : The version of the protocol used messageID : A unique ID for the message stationID : The ID of the station that captured the CAM","title":"Header"},{"location":"datasets-description/unimore-cam-description/#cam","text":"generationDeltaTime : Time since last CAM message","title":"CAM"},{"location":"datasets-description/unimore-cam-description/#basiccontainer","text":"stationType : The type of the station that captured the CAM referencePosition : latitude : The latitude of the station longitude : The longitude of the station positionConfidenceEllipse : Confidence ellipse of the position semiMajorConfidence : Major axis confidence semiMinorConfidence : Minor axis confidence semiMajorOrientation : Major axis orientation altitude : altitudeValue : The altitude value altitudeConfidence : Confidence in the altitude","title":"basicContainer"},{"location":"datasets-description/unimore-cam-description/#highfrequencycontainer","text":"heading : headingValue : The heading value headingConfidence : Confidence in the heading value speed : speedValue : The speed value speedConfidence : Confidence in the speed value driveDirection : The drive direction (forward, backward,unavailable) vehicleLength : vehicleLengthValue : The length of the vehicle vehicleLengthConfidenceIndication : Confidence in the vehicle length vehicleWidth : The width of the vehicle longitudinalAcceleration : longitudinalAccelerationValue : The longitudinal acceleration value longitudinalAccelerationConfidence : Confidence in the longitudinal acceleration curvature : curvatureValue : The curvature value curvatureConfidence : Confidence in the curvature value curvatureCalculationMode : How the curvature was calculated yawRate : yawRateValue : The yaw rate value yawRateConfidence : Confidence in the yaw rate value accelerationControl : Acceleration control status lanePosition : The lane position of the vehicle","title":"highFrequencyContainer"},{"location":"datasets-description/unimore-dms-description/","text":"DMS Dataset The DMS dataset contains messages with information about Driving Monitoring System (DMS) detected. Header protocolVersion : The version of the protocol used messageID : A unique ID for the message stationID : The ID of the station that detected the DMS DMS timestamp : The timestamp when the DMS was detected referencePosition : latitude : The latitude of the location of the DMS longitude : The longitude of the location of the DMS dms_level : The level of severity of the DMS (1 to 5) dms_trigger : The trigger that detected the DMS (e.g. sudden braking, swerving, drowsy.. etc.)","title":"DMS Dataset"},{"location":"datasets-description/unimore-dms-description/#dms-dataset","text":"The DMS dataset contains messages with information about Driving Monitoring System (DMS) detected.","title":"DMS Dataset"},{"location":"datasets-description/unimore-dms-description/#header","text":"protocolVersion : The version of the protocol used messageID : A unique ID for the message stationID : The ID of the station that detected the DMS","title":"Header"},{"location":"datasets-description/unimore-dms-description/#dms","text":"timestamp : The timestamp when the DMS was detected referencePosition : latitude : The latitude of the location of the DMS longitude : The longitude of the location of the DMS dms_level : The level of severity of the DMS (1 to 5) dms_trigger : The trigger that detected the DMS (e.g. sudden braking, swerving, drowsy.. etc.)","title":"DMS"},{"location":"datasets-description/unimore-masa-description/","text":"MASA Dataset The MASA dataset contains messages with information about road users (vehicles, pedestrians, etc.) detected in Modena, Italy. Header protocolVersion : The version of the protocol used. messageID : A unique ID for the message stationID : The ID of the station that detected the road users. MASA Parameters source_idx : The index of the detector that captured the information. timestamp : The timestamp when the road users were detected. Road Users category : The category of the road user (e.g. car, pedestrian, etc.) latitude : The latitude of the road user longitude : The longitude of the road user speed : The speed of the road user in km/h. orientation : The orientation of the road user in degrees. cam_id : The ID of the camera that detected the road user. object_id : A unique ID for the road user.","title":"MASA Dataset"},{"location":"datasets-description/unimore-masa-description/#masa-dataset","text":"The MASA dataset contains messages with information about road users (vehicles, pedestrians, etc.) detected in Modena, Italy.","title":"MASA Dataset"},{"location":"datasets-description/unimore-masa-description/#header","text":"protocolVersion : The version of the protocol used. messageID : A unique ID for the message stationID : The ID of the station that detected the road users.","title":"Header"},{"location":"datasets-description/unimore-masa-description/#masa","text":"","title":"MASA"},{"location":"datasets-description/unimore-masa-description/#parameters","text":"source_idx : The index of the detector that captured the information. timestamp : The timestamp when the road users were detected.","title":"Parameters"},{"location":"datasets-description/unimore-masa-description/#road-users","text":"category : The category of the road user (e.g. car, pedestrian, etc.) latitude : The latitude of the road user longitude : The longitude of the road user speed : The speed of the road user in km/h. orientation : The orientation of the road user in degrees. cam_id : The ID of the camera that detected the road user. object_id : A unique ID for the road user.","title":"Road Users"},{"location":"datasets-description/unimore-pld-description/","text":"Parking Lot Detector (PLD) Description PLD uses deep learning and computer vision algorithms to detect parking spaces from infrastructure/traffic cameras Data Fields Description In this section we will understand the PLD data in further details. PLD data schema can be found here here PLD data message starts with a header section which is standard across all the platform, but has a unique messageID { \"header\": { \"protocolVersion\": 2, \"messageID\": 41, \"stationID\": 3907 }, The actual PLD message starts from this key/value pair pld : \"pld\": { \"sourceid\": 92050, \"sourcegps\": [40487111,79494789], \"timestamp\": 1684077089, \"spaces\": [ { \"spaceid\": 0, \"space\": [ 44656399, 10929777 ], \"prediction\": 0 }, { \"spaceid\": 1, \"space\": [ 44656526, 10929629 ], \"prediction\": 1 } ] } sourceid : Refers to the unique camera ID of the infrastructure camera sourcegps : Refers to the GPS position of the infrastructure camera [latitude,longitude] timestamp : This field refers to the timestamp when the PLD detections were generated. spaces : This is a list of key/value pairs of the detected parking spaces. Every parking space in here has three parameters: spaceid : Unique identifier for each parking space. space : The GPS position ([lat, long]) of the centroid of the parking space. prediction : This is a binary value determining the current occupancy status of the parking space. 0 stands for a free spot and 1 is for an occupied parking space.","title":"Unimore pld description"},{"location":"datasets-description/unimore-pld-description/#parking-lot-detector-pld-description","text":"PLD uses deep learning and computer vision algorithms to detect parking spaces from infrastructure/traffic cameras","title":"Parking Lot Detector (PLD) Description"},{"location":"datasets-description/unimore-pld-description/#data-fields-description","text":"In this section we will understand the PLD data in further details. PLD data schema can be found here here PLD data message starts with a header section which is standard across all the platform, but has a unique messageID { \"header\": { \"protocolVersion\": 2, \"messageID\": 41, \"stationID\": 3907 }, The actual PLD message starts from this key/value pair pld : \"pld\": { \"sourceid\": 92050, \"sourcegps\": [40487111,79494789], \"timestamp\": 1684077089, \"spaces\": [ { \"spaceid\": 0, \"space\": [ 44656399, 10929777 ], \"prediction\": 0 }, { \"spaceid\": 1, \"space\": [ 44656526, 10929629 ], \"prediction\": 1 } ] } sourceid : Refers to the unique camera ID of the infrastructure camera sourcegps : Refers to the GPS position of the infrastructure camera [latitude,longitude] timestamp : This field refers to the timestamp when the PLD detections were generated. spaces : This is a list of key/value pairs of the detected parking spaces. Every parking space in here has three parameters: spaceid : Unique identifier for each parking space. space : The GPS position ([lat, long]) of the centroid of the parking space. prediction : This is a binary value determining the current occupancy status of the parking space. 0 stands for a free spot and 1 is for an occupied parking space.","title":"Data Fields Description"},{"location":"datasets-description/versailles_cam_description/","text":"CAM Messages of a simulation which is running during 1 hour with few hundreds of vehicles. The CAM dataset contains messages with information captured by Cooperative Awareness Messages (CAM). Relevant information in each message is: StationID, generationDeltaTime, latitude, longitude, speed Header protocolVersion: The version of the protocol used messageID: A unique ID for the message stationID: The ID of the station that captured the CAM CAM generationDeltaTime: enerationDeltaTime = TimestampIts mod 65 536 basicContainer stationType: The type of the station that captured the CAM referencePosition: latitude: The latitude of the station longitude: The longitude of the station highFrequencyContainer heading: headingValue: The heading value speed: speedValue: The speed value driveDirection: The drive direction (forward, backward,unavailable) vehicleLength: vehicleLengthValue: The length of the vehicle vehicleLengthConfidenceIndication: Confidence in the vehicle length vehicleWidth: The width of the vehicle longitudinalAcceleration: longitudinalAccelerationValue: The longitudinal acceleration value longitudinalAccelerationConfidence: Confidence in the longitudinal acceleration accelerationControl: Acceleration control status lanePosition: The lane position of the vehicle","title":"Versailles cam description"},{"location":"datasets-description/versailles_pam_description/","text":"Parking Avaialability Messages (PAM) of 2 parking areas in a simulation which is running during 1 hour. Relevant information in each message is: parkingID, capacity, occupancy, Header protocolVersion: The version of the protocol used messageID: A unique ID for the message stationID: The ID of the station that captured the CAM PAM Timestamp: represents the simulation step parking: id: Unique identifier of the parking area capacity: Total number of avaialable paring slots occupancy: Number of occupied parking slots","title":"Versailles pam description"},{"location":"datasets-description/versailles_spat_description/","text":"Signal Phase and Timing (SPaT) Messages of 3 intersection in a simulation which is running during 1 hour. Relevant information in each message is: IntersectionID, LaneId (sub_id), State, Header protocolVersion: The version of the protocol used messageID: A unique ID for the message stationID: The ID of the station that captured the CAM SPAT Timestamp: represents the simulation step Intersections: List of Intersection covered by SPaT messages Intersection Id: Unique Intersection identifier States: List of state for each traffic signal line State sub_id: Identifier of traffic signal line state: Current state of traffic signal","title":"Versailles spat description"},{"location":"datasets-description/versailles_traffic_sensors_description/","text":"Live data colleted from 5 traffic sensors installed in the city of Versailles Relevant information in each traffic sensors is: Time, Throughput, OccupancyRate, Header protocolVersion: The version of the protocol used messageID: A unique ID for the message stationID: The ID of the station that captured the CAM Traffic_sensors List current observation for every traffic sensors Traffic sensor Throughput: Number of vehicles detected every 5 minutes OccupationRate: Sum of the travel time for each vehicle for a given time window Time: Time of the measurement done by the sensor","title":"Versailles traffic sensors description"},{"location":"datasets-description/vicomtech_cam_description/","text":"CAM Messages of a simulation with maximun of 50 cars at a time. The CAM dataset contains messages with information captured by Cooperative Awareness Messages (CAM). Relevant information in each message is: StationID, generationDeltaTime, latitude, longitude, heading, acceleration, speed Header protocolVersion: The version of the protocol used messageID: A unique ID for the message stationID: The ID of the station that captured the CAM CAM generationDeltaTime: enerationDeltaTime = TimestampIts mod 65 536 TimestampIts represents an integer value in milliseconds since 2004-01-01T00:00:00:000Z basicContainer stationType: The type of the station that captured the CAM referencePosition: latitude: The latitude of the station longitude: The longitude of the station positionConfidenceEllipse: Confidence ellipse of the position semiMajorConfidence: Major axis confidence semiMinorConfidence: Minor axis confidence semiMajorOrientation: Major axis orientation altitude: altitudeValue: The altitude value altitudeConfidence: Confidence in the altitude highFrequencyContainer heading: headingValue: The heading value headingConfidence: Confidence in the heading value speed: speedValue: The speed value speedConfidence: Confidence in the speed value driveDirection: The drive direction (forward, backward,unavailable) vehicleLength: vehicleLengthValue: The length of the vehicle vehicleLengthConfidenceIndication: Confidence in the vehicle length vehicleWidth: The width of the vehicle longitudinalAcceleration: longitudinalAccelerationValue: The longitudinal acceleration value longitudinalAccelerationConfidence: Confidence in the longitudinal acceleration curvature: curvatureValue: The curvature value curvatureConfidence: Confidence in the curvature value curvatureCalculationMode: How the curvature was calculated yawRate: yawRateValue: The yaw rate value yawRateConfidence: Confidence in the yaw rate value accelerationControl: Acceleration control status lanePosition: The lane position of the vehicle","title":"Vicomtech cam description"},{"location":"datasets-description/vicomtech_jpg_description/","text":"In vehicle front camera images captured at 2Hz. Camera resolution is 1280x800.Information about position can be found in messages metadata.","title":"Vicomtech jpg description"}]}